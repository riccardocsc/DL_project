{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaf03b25",
   "metadata": {
    "id": "eaf03b25"
   },
   "source": [
    "# HOMEWORK 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dbdff8",
   "metadata": {
    "id": "45dbdff8"
   },
   "source": [
    "## Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6ccf47",
   "metadata": {
    "id": "ad6ccf47"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # plotting library\n",
    "import numpy as np # this module is useful to work with numerical arrays\n",
    "import pandas as pd # this module is useful to work with tabular data\n",
    "import random # this module will be used to select random samples from a collection\n",
    "import os # this module will be used just to create directories in the local filesystem\n",
    "from tqdm import tqdm # this module is useful to plot progress bars\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from torch import nn\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8401fa95",
   "metadata": {
    "id": "8401fa95"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4vWvWtAGcbR8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4vWvWtAGcbR8",
    "outputId": "c60184e5-2ce7-4462-ed73-e635b04492fc"
   },
   "outputs": [],
   "source": [
    "trained = True\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af16c143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the GPU is available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f'Selected device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d742538",
   "metadata": {
    "id": "1d742538"
   },
   "source": [
    "### FashionMNIST Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8996f7ce",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459,
     "referenced_widgets": [
      "0c4533dfc7ea42f98337447a9970fe01",
      "fd3cc333c2cf4230bae890387c2dd003",
      "f9f0710f7bbf45bb973d398edad0c5a6",
      "4ba2ac83b4164aa5b19d6780531e6c92",
      "c572494a9e8c482a9e0b87990e8e24d2",
      "e0e039d792f94b428f50e9257c4da45d",
      "c3f55e96d8a14f2887a370d94b48aa6b",
      "9f18dd4dac38468093f9eba4d98717e9",
      "a1c8d41c6ce44e1cb2883bef5d81a319",
      "02187d9ccaa5436e91d26aabff2f7331",
      "2679dfab50d24d7da95fab0d5510d3a5",
      "936ab4d04e3940f0a0a20387af830ef1",
      "7f69ec1c0c134003b3b3f2dfe61cb804",
      "fc2393a9331b4243bc8b85ed0693b66b",
      "5ff0b1e962334e35b1dc5af8ab929121",
      "2696f56eecab4d22a9f2375638996d51",
      "f745645620394813896ac7bf849c0b8c",
      "cfd333b52a554a2b83e1b0a16f79bd95",
      "c19d3c06a1dc4632ad9291f7824ceb54",
      "6389d9a3c9b3486eba066e188ea847e8",
      "73dc231b525144c983002ef511b3657a",
      "a90c98c4f2004de3972eeb8cb3c37959",
      "9fc8fcf86d72424188c5883c0c77009f",
      "affcab26790d4b969a3f6a59d0c8a2f4",
      "d88c41fbc9654084a253b6618c8cab2b",
      "c1d5274e75f24356b3dd4beb0f7417bd",
      "ec264e85179f4107bfc4babd064770c6",
      "0b535b199f5d49ed861efe5b3d181bb2",
      "1022c1848e6a404dbd4f0dd6251d417d",
      "e5db4b9598c248a6b3036a12c899f203",
      "2b3bcc308f3f4d5eb4548bd98d2b9f19",
      "0c7c8a6976d24c98b34b28781050b976",
      "3503e32bc3304c4db6e6623cb54a6bf1",
      "609ca3add2a449008d492c8f6df622e2",
      "779c56e2affe428faa1be4db4dbdc4ed",
      "0e54025c37444b8198a77d7be5236415",
      "845d3bf4de304446a82bd5803d287f38",
      "075ff3fbfa9e4bb690e1274bdef7d440",
      "3e762c4342c84b7b9b3122f09adc3e70",
      "56cddf80f3c4419e997d8e2914b4bd57",
      "3a26c161c4494eee9af30ba08e5319dd",
      "2e274f81dce14236a25c0c60ed84d7dc",
      "1f49bf7d056f40da9058485e9d85b677",
      "9f09f15ae7db41d0885ea9caeab875c6"
     ]
    },
    "id": "8996f7ce",
    "outputId": "e7939de4-c409-4a45-e201-cad9463638f9"
   },
   "outputs": [],
   "source": [
    "### Download the data and create dataset\n",
    "data_dir = 'classifier_data'\n",
    "# With these commands the train and test datasets, respectively, are downloaded \n",
    "# automatically and stored in the local \"data_dir\" directory.\n",
    "train_dataset = torchvision.datasets.FashionMNIST(data_dir, train=True, download=True)\n",
    "test_dataset  = torchvision.datasets.FashionMNIST(data_dir, train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6208c621",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 559
    },
    "id": "6208c621",
    "outputId": "99ae8a4d-0853-4b57-baaf-2b15aadf7620"
   },
   "outputs": [],
   "source": [
    "### Plot some sample\n",
    "label_names=['t-shirt/top','trouser','pullover','dress','coat','sandal','shirt',\n",
    "             'sneaker','bag','ankle boot']\n",
    "fig, axs = plt.subplots(5, 5, figsize=(8,8))\n",
    "for ax in axs.flatten():\n",
    "    # random.choice allows to randomly sample from a list-like object (basically anything that can be accessed with an index, like our dataset)\n",
    "    img, label = random.choice(train_dataset)\n",
    "    ax.imshow(np.array(img), cmap='gist_gray')\n",
    "    ax.set_title(f'Label: {label_names[label]} [{label}]')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6aab773",
   "metadata": {
    "id": "c6aab773"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e486fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "68e486fd",
    "outputId": "c12fc9bc-9067-4687-f10a-9dd4264d95ec"
   },
   "outputs": [],
   "source": [
    "# Chek how looks like the dataset before transform\n",
    "print(train_dataset[0][0])\n",
    "\n",
    "# Set the train transform\n",
    "train_dataset.transform = transform\n",
    "# Set the test transform\n",
    "test_dataset.transform = transform\n",
    "\n",
    "# Chek how looks like the dataset after transform\n",
    "print(train_dataset[0][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ee698a",
   "metadata": {},
   "source": [
    "### Train/Validation/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5292b1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cd5292b1",
    "outputId": "af738a92-54e8-45c6-95a7-9d238df008b7"
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "validation_split = .2\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(train_dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "# Define train dataloader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "# Define validation dataloader\n",
    "val_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=valid_sampler, shuffle = False)\n",
    "# Define test dataloader\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle = False)\n",
    "\n",
    "batch_data, batch_labels = next(iter(train_dataloader))\n",
    "print(f\"TRAIN BATCH SHAPE\")\n",
    "print(f\"\\t Data: {batch_data.shape}\")\n",
    "print(f\"\\t Labels: {batch_labels.shape}\")\n",
    "\n",
    "batch_data, batch_labels = next(iter(val_dataloader))\n",
    "print(f\"TRAIN BATCH SHAPE\")\n",
    "print(f\"\\t Data: {batch_data.shape}\")\n",
    "print(f\"\\t Labels: {batch_labels.shape}\")\n",
    "\n",
    "batch_data, batch_labels = next(iter(test_dataloader))\n",
    "print(f\"TEST BATCH SHAPE\")\n",
    "print(f\"\\t Data: {batch_data.shape}\")\n",
    "print(f\"\\t Labels: {batch_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dec27d",
   "metadata": {
    "id": "74dec27d"
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e66f8e",
   "metadata": {
    "id": "75e66f8e"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoded_space_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        ### Convolutional section\n",
    "        self.encoder_cnn = nn.Sequential(\n",
    "            # First convolutional layer   [16x15x15]\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=2, padding=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm2d(16),\n",
    "            \n",
    "            # Second convolutional layer  [32x6x6]\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=2, padding=0),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            \n",
    "            # Third convolutional layer   [64x3x3]\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm2d(64)\n",
    "        )\n",
    "        \n",
    "        ### Flatten layer\n",
    "        self.flatten = nn.Flatten(start_dim=1) #1 to exclude the batch size\n",
    "\n",
    "        ### Linear section\n",
    "        self.encoder_lin = nn.Sequential(\n",
    "            # First linear layer\n",
    "            nn.Linear(in_features=64*3*3, out_features=64),\n",
    "            nn.ReLU(True),\n",
    "            # Second linear layer\n",
    "            nn.Linear(in_features=64, out_features=encoded_space_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply convolutions\n",
    "        x = self.encoder_cnn(x)\n",
    "        # Flatten\n",
    "        x = self.flatten(x)\n",
    "        # # Apply linear layers\n",
    "        x = self.encoder_lin(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5723b30",
   "metadata": {
    "id": "d5723b30"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6903ced",
   "metadata": {
    "id": "e6903ced"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoded_space_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        ### Linear section\n",
    "        self.decoder_lin = nn.Sequential(\n",
    "            # First linear layer\n",
    "            nn.Linear(in_features = encoded_space_dim, out_features=64),\n",
    "            nn.ReLU(True),\n",
    "            # Second linear layer\n",
    "            nn.Linear(in_features=64, out_features=64*3*3),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.unflatten = nn.Unflatten(dim=1, unflattened_size=(64, 3, 3))\n",
    "\n",
    "        ### Convolutional section\n",
    "        self.decoder_conv = nn.Sequential(\n",
    "            # First transposed convolution  [32x6x6]\n",
    "            nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            \n",
    "            # Second transposed convolution  [16x14x14]\n",
    "            nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=5, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm2d(16),\n",
    "            \n",
    "            # Third transposed convolution   [1x28x28]\n",
    "            nn.ConvTranspose2d(in_channels=16, out_channels=1, kernel_size=5, stride=2, padding=2, output_padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply linear layers\n",
    "        x = self.decoder_lin(x)\n",
    "        # Unflatten\n",
    "        x = self.unflatten(x)\n",
    "        # Apply transposed convolutions\n",
    "        x = self.decoder_conv(x)\n",
    "        # Apply a sigmoid to force the output to be between 0 and 1 (valid pixel values)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d491c9db",
   "metadata": {
    "id": "d491c9db"
   },
   "outputs": [],
   "source": [
    "### Initialize the two networks\n",
    "encoded_space_dim = 20\n",
    "encoder = Encoder(encoded_space_dim=encoded_space_dim)\n",
    "decoder = Decoder(encoded_space_dim=encoded_space_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3a4c78",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cc3a4c78",
    "outputId": "409f633a-2006-47bf-fd8d-e510ed3c48c2"
   },
   "outputs": [],
   "source": [
    "### Some examples\n",
    "# Take an input image \n",
    "img, _ = test_dataset[0]\n",
    "img = img.unsqueeze(0) # Add the batch dimension in the first axis\n",
    "print('Original image shape:', img.shape)\n",
    "# Encode the image\n",
    "img_enc = encoder(img)\n",
    "print('Encoded image shape:', img_enc.shape)\n",
    "# Decode the image\n",
    "dec_img = decoder(img_enc)\n",
    "print('Decoded image shape:', dec_img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab40b18",
   "metadata": {
    "id": "0ab40b18"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8f187b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bd8f187b",
    "outputId": "a08647d7-1b07-4638-ae2d-d3cdbb743204"
   },
   "outputs": [],
   "source": [
    "### Define the loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "### Define an optimizer (both for the encoder and the decoder!)\n",
    "lr = 0.01 # Learning rate\n",
    "params_to_optimize = [\n",
    "    {'params': encoder.parameters()},\n",
    "    {'params': decoder.parameters()}\n",
    "]\n",
    "#optim = torch.optim.Adam(params_to_optimize, lr=lr, weight_decay=1e-5)\n",
    "#scheduler = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.9)\n",
    "\n",
    "optim = torch.optim.SGD(params_to_optimize, lr=lr)\n",
    "\n",
    "# Move both the encoder and the decoder to the selected device\n",
    "encoder.to(device)\n",
    "decoder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fcacb9",
   "metadata": {
    "id": "d9fcacb9"
   },
   "source": [
    "### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43b3264",
   "metadata": {
    "id": "b43b3264"
   },
   "outputs": [],
   "source": [
    "### Training function\n",
    "def train_epoch(encoder, decoder, device, dataloader, loss_fn, optimizer, scheduler, printer=False):\n",
    "    # Set train mode for both the encoder and the decoder\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    train_loss = []\n",
    "    # Iterate the dataloader\n",
    "    for image_batch, _ in dataloader:\n",
    "        # Move tensor to the proper device\n",
    "        image_batch = image_batch.to(device)\n",
    "        # Encode data\n",
    "        encoded_data = encoder(image_batch)\n",
    "        # Decode data\n",
    "        decoded_data = decoder(encoded_data)\n",
    "        # Evaluate loss\n",
    "        loss = loss_fn(decoded_data, image_batch)\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.item())\n",
    "    if scheduler != None:\n",
    "        scheduler.step()\n",
    "    # Print train loss\n",
    "    if(printer):\n",
    "        print('\\t train loss: %f' % (np.mean(train_loss)))\n",
    "    return np.mean(train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ccfab4",
   "metadata": {
    "id": "04ccfab4"
   },
   "source": [
    "### Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6150e936",
   "metadata": {
    "id": "6150e936"
   },
   "outputs": [],
   "source": [
    "### Testing function\n",
    "def test_epoch(encoder, decoder, device, dataloader, loss_fn):\n",
    "    # Set evaluation mode for encoder and decoder\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    with torch.no_grad(): # No need to track the gradients\n",
    "        # Define the lists to store the outputs for each batch\n",
    "        conc_out = []\n",
    "        conc_label = []\n",
    "        for image_batch, _ in dataloader:\n",
    "            # Move tensor to the proper device\n",
    "            image_batch = image_batch.to(device)\n",
    "            # Encode data\n",
    "            encoded_data = encoder(image_batch)\n",
    "            # Decode data\n",
    "            decoded_data = decoder(encoded_data)\n",
    "            # Append the network output and the original image to the lists\n",
    "            conc_out.append(decoded_data.cpu())\n",
    "            conc_label.append(image_batch.cpu())\n",
    "        # Create a single tensor with all the values in the lists\n",
    "        conc_out = torch.cat(conc_out)\n",
    "        conc_label = torch.cat(conc_label) \n",
    "        # Evaluate global loss\n",
    "        test_loss = loss_fn(conc_out, conc_label)\n",
    "    return test_loss.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ELGRQjH4VJXR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "id": "ELGRQjH4VJXR",
    "outputId": "e782db09-f3d3-45d0-e011-16c9df42c5d5"
   },
   "outputs": [],
   "source": [
    "if not trained:\n",
    "    ### Training cycle\n",
    "    train_loss_log = []\n",
    "    val_loss_log = []\n",
    "\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        print('EPOCH %d/%d' % (epoch + 1, num_epochs))\n",
    "        ### Training (use the training function)\n",
    "        train_loss = train_epoch(\n",
    "            encoder=encoder, \n",
    "            decoder=decoder, \n",
    "            device=device, \n",
    "            dataloader=train_dataloader, \n",
    "            loss_fn=loss_fn, \n",
    "            optimizer=optim,\n",
    "            scheduler=None, \n",
    "            printer=True)\n",
    "        train_loss_log.append(train_loss)\n",
    "        ### Validation  (use the testing function)\n",
    "        val_loss = test_epoch(\n",
    "            encoder=encoder, \n",
    "            decoder=decoder, \n",
    "            device=device, \n",
    "            dataloader=test_dataloader, \n",
    "            loss_fn=loss_fn)\n",
    "        val_loss_log.append(val_loss)\n",
    "        # Print Validationloss\n",
    "        print('\\n\\n\\t VALIDATION - EPOCH %d/%d - loss: %f\\n\\n' % (epoch + 1, num_epochs, val_loss))\n",
    "\n",
    "        ### Plot progress\n",
    "        # Get the output of a specific image (the test image at index 0 in this case)\n",
    "        img = test_dataset[0][0].unsqueeze(0).to(device)\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        with torch.no_grad():\n",
    "            rec_img  = decoder(encoder(img))\n",
    "        # Plot the reconstructed image\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(12,6))\n",
    "        axs[0].imshow(img.cpu().squeeze().numpy(), cmap='gist_gray')\n",
    "        axs[0].set_title('Original image')\n",
    "        axs[1].imshow(rec_img.cpu().squeeze().numpy(), cmap='gist_gray')\n",
    "        axs[1].set_title('Reconstructed image (EPOCH %d)' % (epoch + 1))\n",
    "        plt.tight_layout()\n",
    "        plt.pause(0.1)\n",
    "        # Save figures\n",
    "        os.makedirs('autoencoder_progress_%d_features' % encoded_space_dim, exist_ok=True)\n",
    "        fig.savefig('autoencoder_progress_%d_features/epoch_%d.jpg' % (encoded_space_dim, epoch + 1))\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        # Save network parameters\n",
    "        torch.save(encoder.state_dict(), 'encoder_params_simple.pth')\n",
    "        torch.save(decoder.state_dict(), 'decoder_params_simple.pth')\n",
    "else:\n",
    "    # Load network parameters\n",
    "    encoder_state_dict = torch.load('encoder_params_simple.pth', map_location=device)\n",
    "    decoder_state_dict = torch.load('decoder_params_simple.pth', map_location=device)\n",
    "    \n",
    "    encoder.load_state_dict(encoder_state_dict)\n",
    "    decoder.load_state_dict(decoder_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac66a233",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some examples of reconstructed images\n",
    "indices = np.random.randint(len(test_dataset), size=8)\n",
    "subset = torch.utils.data.Subset(test_dataset, indices)\n",
    "testloader_subset = DataLoader(subset, batch_size=1, num_workers=0, shuffle=False)\n",
    "    \n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "fig, axs = plt.subplots(2, 4, figsize=(15,15))\n",
    "axs = axs.flatten()\n",
    "ax_n = 0\n",
    "## Iterate trough the samples in the test dataset\n",
    "iterator = iter(testloader_subset)\n",
    "loop = True\n",
    "while loop:\n",
    "    try:\n",
    "        data, label = next(iterator)\n",
    "    except StopIteration:\n",
    "        loop = False\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            latent = encoder(data.to(device))\n",
    "            out = decoder(latent)\n",
    "        axs[ax_n].set_xticks([])\n",
    "        axs[ax_n].set_yticks([])\n",
    "        axs[ax_n].imshow(out.cpu().squeeze().numpy(), cmap='gist_gray')\n",
    "        axs[ax_n].set_title(label_names[label[0].item()], fontsize = 18)\n",
    "        ax_n += 1\n",
    "        plt.tight_layout()\n",
    "#plt.savefig(\"simple_reconstruction_examples.pdf\", format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095dca5c",
   "metadata": {
    "id": "095dca5c"
   },
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555e7ae9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "555e7ae9",
    "outputId": "466e7c5a-16f1-4b02-dd73-71bb35d87c50"
   },
   "outputs": [],
   "source": [
    "if not trained:\n",
    "        ###########OPTIMIZATION###########\n",
    "        ! pip install optuna\n",
    "        import optuna\n",
    "        from optuna.integration import PyTorchLightningPruningCallback\n",
    "        EPOCHS = 10\n",
    "\n",
    "        def objective(trial):\n",
    "\n",
    "            encoded_space_dim = 20\n",
    "            encoder = Encoder(encoded_space_dim=encoded_space_dim).to(device)\n",
    "            decoder = Decoder(encoded_space_dim=encoded_space_dim).to(device)\n",
    "\n",
    "            parameters = [{'params': encoder.parameters()}, {'params': decoder.parameters()}]\n",
    "\n",
    "            # Type of optimizer algorithm\n",
    "            optim_name = ['Adam', 'SGD', 'Adagrad', 'RMSprop']\n",
    "            optim_algorithm = trial.suggest_categorical(\"optimizer\", optim_name)\n",
    "\n",
    "            # Weight decay \n",
    "            w_decay = trial.suggest_float(\"weight_decay\", 0, 1e-6)\n",
    "\n",
    "            # Scheduling factor\n",
    "            gamma = trial.suggest_float(\"gamma\", 0, 1)\n",
    "\n",
    "            optimizer = getattr(torch.optim, optim_algorithm)(parameters, lr = 0.01, weight_decay=w_decay)\n",
    "            scheduler.gamma = gamma\n",
    "\n",
    "            loss_func = nn.MSELoss()\n",
    "\n",
    "            for epoch in range(EPOCHS):\n",
    "                loss = train_epoch(encoder, decoder, device, train_dataloader, loss_fn, optimizer, scheduler)\n",
    "                test_epoch(encoder, decoder, device, val_dataloader, loss_fn)\n",
    "\n",
    "                trial.report(loss, epoch)\n",
    "\n",
    "                # Handle pruning based on the intermediate value.\n",
    "                if trial.should_prune():\n",
    "                    raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "            return loss\n",
    "\n",
    "        pruner: optuna.pruners.BasePruner = optuna.pruners.MedianPruner(n_startup_trials=3, n_warmup_steps=0, interval_steps=1)\n",
    "\n",
    "        study = optuna.create_study(study_name=\"Encoder\", direction=\"minimize\", pruner=pruner)\n",
    "        study.optimize(objective, n_trials=50)\n",
    "\n",
    "        print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "        print(\"Best trial:\")\n",
    "        trial = study.best_trial\n",
    "\n",
    "        print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "        print(\"  Params: \")\n",
    "        for key, value in trial.params.items():\n",
    "            print(\"    {}: {}\".format(key, value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97gwqkVTFIZ",
   "metadata": {
    "id": "d97gwqkVTFIZ"
   },
   "outputs": [],
   "source": [
    "print('#############')\n",
    "print(\"BEST TRIAL\")\n",
    "print('#############')\n",
    "print(\"\\nParameters:\")\n",
    "print(\"weight_decay: 2.9505970645836693e-05\")\n",
    "print(\"gamma: 0.495944901064486\")\n",
    "print(\"optimizer: Adagrad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cj1N8DexUjZ4",
   "metadata": {
    "id": "cj1N8DexUjZ4"
   },
   "outputs": [],
   "source": [
    "### Initialize the two networks\n",
    "encoded_space_dim = 20\n",
    "encoder = Encoder(encoded_space_dim=encoded_space_dim).to(device)\n",
    "decoder = Decoder(encoded_space_dim=encoded_space_dim).to(device)\n",
    "\n",
    "### Define an optimizer (both for the encoder and the decoder!)\n",
    "lr = 0.01 # Learning rate\n",
    "params_to_optimize = [\n",
    "    {'params': encoder.parameters()},\n",
    "    {'params': decoder.parameters()}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16284ecc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "16284ecc",
    "outputId": "0bffa5a4-68af-4304-affa-998dbc637481"
   },
   "outputs": [],
   "source": [
    "if not trained:\n",
    "    ### Training cycle\n",
    "    train_loss_log = []\n",
    "    val_loss_log = []\n",
    "    lr_log = []\n",
    "    best_loss = np.infty\n",
    "\n",
    "    optim = torch.optim.Adagrad(params_to_optimize, lr=lr, weight_decay=2.9505970645836693e-05)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.495944901064486)\n",
    "\n",
    "    num_epochs = 50\n",
    "    patience = 3\n",
    "    for epoch in range(num_epochs):\n",
    "        print('EPOCH %d/%d' % (epoch + 1, num_epochs))\n",
    "        ### Training (use the training function)\n",
    "        train_loss = train_epoch(\n",
    "            encoder=encoder, \n",
    "            decoder=decoder, \n",
    "            device=device, \n",
    "            dataloader=train_dataloader, \n",
    "            loss_fn=loss_fn, \n",
    "            optimizer=optim,\n",
    "            scheduler=scheduler)\n",
    "        train_loss_log.append(train_loss)\n",
    "        lr_log.append(scheduler.get_last_lr())\n",
    "        ### Validation  (use the testing function)\n",
    "        val_loss = test_epoch(\n",
    "            encoder=encoder, \n",
    "            decoder=decoder, \n",
    "            device=device, \n",
    "            dataloader=test_dataloader, \n",
    "            loss_fn=loss_fn)\n",
    "        val_loss_log.append(val_loss)\n",
    "        # Print Validationloss\n",
    "        print('\\n\\n\\t VALIDATION - EPOCH %d/%d - loss: %f\\n\\n' % (epoch + 1, num_epochs, val_loss))\n",
    "\n",
    "        ### Plot progress\n",
    "        # Get the output of a specific image (the test image at index 0 in this case)\n",
    "        img = test_dataset[0][0].unsqueeze(0).to(device)\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        with torch.no_grad():\n",
    "            rec_img  = decoder(encoder(img))\n",
    "        # Plot the reconstructed image\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(12,6))\n",
    "        axs[0].imshow(img.cpu().squeeze().numpy(), cmap='gist_gray')\n",
    "        axs[0].set_title('Original image')\n",
    "        axs[1].imshow(rec_img.cpu().squeeze().numpy(), cmap='gist_gray')\n",
    "        axs[1].set_title('Reconstructed image (EPOCH %d)' % (epoch + 1))\n",
    "        plt.tight_layout()\n",
    "        plt.pause(0.1)\n",
    "        # Save figures\n",
    "        os.makedirs('autoencoder_progress_%d_features' % encoded_space_dim, exist_ok=True)\n",
    "        fig.savefig('autoencoder_progress_%d_features/epoch_%d.jpg' % (encoded_space_dim, epoch + 1))\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        # Save network parameters\n",
    "        torch.save(encoder.state_dict(), 'encoder_params.pth')\n",
    "        torch.save(decoder.state_dict(), 'decoder_params.pth')\n",
    "\n",
    "        # Implement early stopping\n",
    "        if(val_loss_log[-1] < best_loss):\n",
    "            best_loss = val_loss_log[-1]\n",
    "            patience = 3\n",
    "        else:\n",
    "            patience -= 1\n",
    "            if(patience == 0): \n",
    "                print(\"#################\\nLearning stopped because the validation error was not improving\\n#################\")\n",
    "                break\n",
    "else:\n",
    "    # Load network parameters\n",
    "    encoder_state_dict = torch.load('encoder_params.pth', map_location=device)\n",
    "    decoder_state_dict = torch.load('decoder_params.pth', map_location=device)\n",
    "    \n",
    "    encoder.load_state_dict(encoder_state_dict)\n",
    "    decoder.load_state_dict(decoder_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73129840",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "id": "73129840",
    "outputId": "d8324f73-3686-4b18-9edf-c9134b252276"
   },
   "outputs": [],
   "source": [
    "if not trained:\n",
    "    ## plot the reconstruction loss\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.semilogy(train_loss_log, label='Train loss')\n",
    "    plt.semilogy(val_loss_log, label='Validation loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RpGZShvVf0rv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "id": "RpGZShvVf0rv",
    "outputId": "8712d8bb-82c5-4d72-af8f-4b877c0a3b4b"
   },
   "outputs": [],
   "source": [
    "if not trained:  \n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.plot(lr_log)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Learning rate')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab951a5f",
   "metadata": {},
   "source": [
    "## Network Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a28ce5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "id": "c6a28ce5",
    "outputId": "9e8a4904-6899-41e8-e4a3-e86475ce97e2"
   },
   "outputs": [],
   "source": [
    "## Some examples of reconstructed images\n",
    "indices = np.random.randint(len(test_dataset), size=8)\n",
    "subset = torch.utils.data.Subset(test_dataset, indices)\n",
    "testloader_subset = DataLoader(subset, batch_size=1, num_workers=0, shuffle=False)\n",
    "    \n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "fig, axs = plt.subplots(2, 4, figsize=(15,15))\n",
    "axs = axs.flatten()\n",
    "ax_n = 0\n",
    "## Iterate trough the samples in the test dataset\n",
    "iterator = iter(testloader_subset)\n",
    "loop = True\n",
    "while loop:\n",
    "    try:\n",
    "        data, label = next(iterator)\n",
    "    except StopIteration:\n",
    "        loop = False\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            latent = encoder(data.to(device))\n",
    "            out = decoder(latent)\n",
    "        axs[ax_n].set_xticks([])\n",
    "        axs[ax_n].set_yticks([])\n",
    "        axs[ax_n].imshow(out.cpu().squeeze().numpy(), cmap='gist_gray')\n",
    "        axs[ax_n].set_title(label_names[label[0].item()])\n",
    "        ax_n += 1\n",
    "        plt.tight_layout()\n",
    "#plt.savefig(\"Examples_adv.pdf\", format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2ba309",
   "metadata": {
    "id": "9e2ba309"
   },
   "source": [
    "### Latent space analysis-PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44e59d9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "id": "f44e59d9",
    "outputId": "27ed97d4-98d8-4927-ba8c-f307f525b24f"
   },
   "outputs": [],
   "source": [
    "### Get the encoded representation of the test samples\n",
    "encoded_samples = []\n",
    "for sample in tqdm(test_dataset):\n",
    "    img = sample[0].unsqueeze(0).to(device)\n",
    "    label = sample[1]\n",
    "    # Encode image\n",
    "    encoder.eval()\n",
    "    with torch.no_grad():\n",
    "        encoded_img  = encoder(img)\n",
    "    # Append to list\n",
    "    encoded_img = encoded_img.flatten().cpu().numpy()\n",
    "    encoded_sample = {f\"Enc. Variable {i}\": enc for i, enc in enumerate(encoded_img)}\n",
    "    encoded_sample['label'] = label\n",
    "    encoded_samples.append(encoded_sample)\n",
    "\n",
    "encoded_samples = pd.DataFrame(encoded_samples)\n",
    "encoded_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b7e634",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "26b7e634",
    "outputId": "9ffb2a92-b597-4140-a537-b7c91707061b"
   },
   "outputs": [],
   "source": [
    "#encoded_samples = encoded_samples.drop('label', axis=1) # this to remove the last column (label)\n",
    "pca = PCA(n_components=2).fit(encoded_samples[:-2])\n",
    "encoded_samples_pca = pca.transform(encoded_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hqtK5gRiCeD1",
   "metadata": {
    "id": "hqtK5gRiCeD1"
   },
   "outputs": [],
   "source": [
    "# This function creates a dictionary in which a key is the numerical label and the corresponding value is the \n",
    "# verbose label\n",
    "def dictionary_classes(label_names):\n",
    "    dic = {}\n",
    "    i = 0\n",
    "    for label in label_names:\n",
    "        dic[str(i)] = label\n",
    "        i += 1\n",
    "    return dic\n",
    "dictionary = dictionary_classes(label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3CzvoHoilDya",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "3CzvoHoilDya",
    "outputId": "22ba1995-0192-41fe-e67e-7d1293ffa643"
   },
   "outputs": [],
   "source": [
    "fig = px.scatter(encoded_samples_pca, x=0, y=1,\n",
    "                 color=encoded_samples.label.astype(str),\n",
    "                 labels={'0': 'Feature 1', '1': 'Feature 2', 'Color': 'Class'})\n",
    "\n",
    "fig.for_each_trace(lambda t: t.update(name = dictionary[t.name]))\n",
    "#fig.write_image(\"PCA.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e546b42",
   "metadata": {
    "id": "5e546b42"
   },
   "source": [
    "### Generate new examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843de505",
   "metadata": {
    "id": "e62f9e45"
   },
   "outputs": [],
   "source": [
    "# Based on the pca find two internal representation\n",
    "t_shirt = np.array([-6, 1]) \n",
    "ankle_boot = np.array([6, -2])\n",
    "sample1 = torch.tensor(np.dot(t_shirt, pca.components_) + pca.mean_)[:-1]\n",
    "sample2 = torch.tensor(np.dot(ankle_boot, pca.components_) + pca.mean_)[:-1]\n",
    "samples = [sample1, sample2]\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "encoder.float()\n",
    "decoder.float()\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15,15))\n",
    "axs = axs.flatten()\n",
    "ax_n = 0\n",
    "while ax_n < 2:\n",
    "    with torch.no_grad():\n",
    "        out = decoder(samples[ax_n].float().unsqueeze(0).to(device)) # add the batch dimension\n",
    "    axs[ax_n].set_xticks([])\n",
    "    axs[ax_n].set_yticks([])\n",
    "    axs[ax_n].imshow(out.cpu().squeeze().numpy(), cmap='gist_gray')\n",
    "    ax_n += 1\n",
    "    plt.tight_layout()\n",
    "#plt.savefig(\"new_examples.pdf\", format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac0c0f9",
   "metadata": {
    "id": "cac0c0f9"
   },
   "source": [
    "## Autoencoder fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c721f9",
   "metadata": {
    "id": "81c721f9"
   },
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder_output):\n",
    "        \n",
    "        super(Classifier,self).__init__()\n",
    "        self.linear = nn.Sequential(nn.Linear(in_features=encoder_output, out_features=512),\n",
    "                                    nn.ReLU(True),\n",
    "                                    nn.BatchNorm1d(512),\n",
    "                                    nn.Linear(in_features=512, out_features=128),\n",
    "                                    nn.ReLU(True),\n",
    "                                    nn.BatchNorm1d(128),\n",
    "                                    nn.Linear(in_features=128, out_features=10))\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fad2a9",
   "metadata": {
    "id": "e7fad2a9"
   },
   "outputs": [],
   "source": [
    "classifier = Classifier(encoder_output=encoded_space_dim)\n",
    "classifier.to(device)\n",
    "encoder = Encoder(encoded_space_dim).to(device)\n",
    "encoder.load_state_dict(torch.load('encoder_params.pth', map_location=device))\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AF9KPUmkg5qY",
   "metadata": {
    "id": "AF9KPUmkg5qY"
   },
   "outputs": [],
   "source": [
    "# Define an optimizer\n",
    "lr = 1e-4\n",
    "optim = torch.optim.Adam(classifier.parameters(), lr=lr, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pwboQ7zGhUtl",
   "metadata": {
    "id": "pwboQ7zGhUtl"
   },
   "source": [
    "### Training and testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r2ZUImAWhTQt",
   "metadata": {
    "id": "r2ZUImAWhTQt"
   },
   "outputs": [],
   "source": [
    "## WE NEED TO UPDATE TRAIN AND TEST FUNCTIONS IN ORDER TO USE THE LABELS THAT \n",
    "## WERE IGNORED IN THE PREVIOUS TWO FUNCTIONS\n",
    "\n",
    "def training_step(encoder, classifier, train_loader, loss_fn, optimizer, train_loss_log, printer=True):\n",
    "        \n",
    "    classifier.train()\n",
    "    train_loss = []\n",
    "    train_correct = 0\n",
    "    for sample_batched in train_loader:\n",
    "        \n",
    "        # Move data to device\n",
    "        x_batch = sample_batched[0].to(device)\n",
    "        label_batch = sample_batched[1].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        x = encoder(x_batch)\n",
    "        out = classifier(x)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(out, label_batch)\n",
    "\n",
    "        # Backpropagation\n",
    "        classifier.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Save train loss for this batch\n",
    "        loss_batch = loss.detach().cpu().numpy()\n",
    "        train_loss.append(loss_batch) \n",
    "        \n",
    "        scores, predictions = torch.max(out.data, 1)\n",
    "        train_correct += (predictions == label_batch).sum().item()\n",
    "        \n",
    "    # Save average train loss over the batches\n",
    "    train_loss = np.mean(train_loss)\n",
    "    if(printer): print(f\"AVERAGE TRAIN LOSS: {train_loss}\")\n",
    "    if(printer): print(f\"TRAINING ACCURACY: {train_correct*100/len(train_loader.sampler)}\")\n",
    "    train_loss_log.append(train_loss)\n",
    "    \n",
    "def validation_step(encoder, classifier, val_loader, loss_fn, val_loss_log, printer = True):\n",
    "\n",
    "    val_loss = []\n",
    "    val_correct = 0\n",
    "    classifier.eval() #evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for sample_batched in val_loader:\n",
    "            x_batch = sample_batched[0].to(device)\n",
    "            label_batch = sample_batched[1].to(device)\n",
    "            \n",
    "            # Predict using the current model\n",
    "            x = encoder(x_batch)\n",
    "            y_pred = classifier(x)\n",
    "            \n",
    "            # Compute and save the val_loss for this batch \n",
    "            loss_batch = loss_fn(y_pred, label_batch).detach().cpu().numpy()\n",
    "            val_loss.append(loss_batch)\n",
    "            \n",
    "            # Accuracy for this batch\n",
    "            scores, predictions = torch.max(y_pred.data, 1)\n",
    "            val_correct += (predictions == label_batch).sum().item()\n",
    "            \n",
    "        # Save average train loss over the batches\n",
    "        val_loss = np.mean(val_loss)\n",
    "        if(printer): print(f\"AVERAGE VALIDATION LOSS: {val_loss}\")\n",
    "        if(printer): print(f\"VALIDATION ACCURACY: {val_correct*100/len(val_loader.sampler)}\")   \n",
    "        val_loss_log.append(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u1lvRo_zh-Ht",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u1lvRo_zh-Ht",
    "outputId": "5efa3a37-3e70-48da-f35c-86611a53b90b"
   },
   "outputs": [],
   "source": [
    "if not trained:\n",
    "    num_epochs = 50\n",
    "    train_loss_log = []\n",
    "    validation_loss_log = []\n",
    "    best_loss = np.infty\n",
    "    patience = 3\n",
    "    for i in range(num_epochs):\n",
    "        print('#################')\n",
    "        print(f'# EPOCH {i}')\n",
    "        print('#################')\n",
    "        #Train pass\n",
    "        training_step(encoder, classifier, train_dataloader, loss_function, optim, train_loss_log, printer=True)\n",
    "        #Validation pass\n",
    "        validation_step(encoder, classifier, val_dataloader, loss_function, validation_loss_log, printer = True)\n",
    "\n",
    "        # Implement early stopping\n",
    "        if(validation_loss_log[-1] < best_loss):\n",
    "            best_loss = validation_loss_log[-1]\n",
    "            patience = 3\n",
    "        else:\n",
    "            patience -= 1\n",
    "            if(patience == 0): \n",
    "                print(\"#################\\nLearning stopped because the validation error was not improving\\n#################\")\n",
    "                break \n",
    "else:\n",
    "    classifier.load_state_dict(torch.load('classifier.torch', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3292a7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(encoder, model, dataloader, loss_fn):\n",
    "    encoder.cpu().eval()\n",
    "    model.cpu().eval()\n",
    "    with torch.no_grad():\n",
    "        labels, outputs = [], []\n",
    "        for image_batch, label_batch in tqdm(dataloader):\n",
    "            out = encoder(image_batch)\n",
    "            x_hat = model(out)\n",
    "            labels.append(label_batch)\n",
    "            outputs.append(x_hat)\n",
    "        labels = torch.cat(labels)\n",
    "        outputs = torch.cat(outputs)\n",
    "        error = loss_fn(outputs, labels)\n",
    "    return outputs, labels, error.data\n",
    "    \n",
    "test_outputs, test_labels, test_loss = test_step(encoder=encoder,\n",
    "    model=classifier,\n",
    "    dataloader=test_dataloader, \n",
    "    loss_fn=nn.CrossEntropyLoss())\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = 0\n",
    "_, predictions = torch.max(test_outputs.data, 1)\n",
    "accuracy += (predictions == test_labels).sum().item()\n",
    "accuracy = accuracy/len(test_dataloader.sampler)*100\n",
    "# Print Test loss\n",
    "print(f\"\\n\\nTEST LOSS : {test_loss}\")\n",
    "# Print accuracy\n",
    "print(f\"\\nTEST ACCURACY : {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24c1997",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import seaborn as sn\n",
    "# Predicted labels\n",
    "y_true = test_labels.cpu().data.numpy()\n",
    "y_pred = test_outputs.cpu().argmax(dim=1).numpy()\n",
    "cm = metrics.confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Convert confusion matrices to pandas data frames\n",
    "CM_df = pd.DataFrame(cm)\n",
    "\n",
    "# Plot confusion matrices\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "ax = sn.heatmap(CM_df, annot=True, cmap='rocket_r', vmax=450, fmt='d')\n",
    "ax.set_xlabel(\"Predicted label\", fontsize=12)\n",
    "ax.set_ylabel(\"True label\", fontsize = 12)\n",
    "#plt.savefig(\"confusion_matrix.pdf\", format='pdf', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6fc079",
   "metadata": {
    "id": "7d6fc079"
   },
   "source": [
    "## Variational autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4edc4f",
   "metadata": {
    "id": "9f4edc4f"
   },
   "outputs": [],
   "source": [
    "####  VARIATIONAL AUTOENCODER ####\n",
    "class VariationalEncoder(nn.Module):\n",
    "    def __init__(self, encoded_space_dim):  \n",
    "        super(VariationalEncoder, self).__init__()\n",
    "        \n",
    "        ### Convolutional section\n",
    "        self.encoder_cnn = nn.Sequential(\n",
    "            # First convolutional layer   [16x15x15]\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=2, padding=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm2d(16),\n",
    "            \n",
    "            # Second convolutional layer  [32x6x6]\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=2, padding=0),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            \n",
    "            # Third convolutional layer   [64x3x3]\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm2d(64)\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.linear1 = nn.Linear(3*3*64, 256)\n",
    "        self.linear2 = nn.Linear(256, 128)\n",
    "        self.mu = nn.Linear(128, encoded_space_dim)  #mean latent vector\n",
    "        self.var = nn.Linear(128, encoded_space_dim)  #var latent vector\n",
    "        \n",
    "        # Activation function\n",
    "        self.act = nn.ReLU(True)\n",
    "\n",
    "        # Flatten layer after the convolutions\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "        \n",
    "        self.log_scale = nn.Parameter(torch.Tensor([0.0]))\n",
    "            \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.encoder_cnn(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.act(self.linear1(x))\n",
    "        x = self.act(self.linear2(x))\n",
    "        mu =  self.mu(x)\n",
    "        var = self.var(x)\n",
    "        return [mu, var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d4868b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "96d4868b",
    "outputId": "e82eeccc-f411-45f0-abe3-778f0c1140f2"
   },
   "outputs": [],
   "source": [
    "# Latent space dimension\n",
    "d = 20\n",
    "\n",
    "var_encoder = VariationalEncoder(encoded_space_dim=d)\n",
    "decoder = Decoder(encoded_space_dim=d) # The decoder class is the same as before\n",
    "\n",
    "lr = 1e-3\n",
    "params_to_optimize = [\n",
    "    {'params': var_encoder.parameters()},\n",
    "    {'params': decoder.parameters()}]\n",
    "optimizer = torch.optim.Adam(params_to_optimize, lr=lr, weight_decay=1e-5)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f'Selected device: {device}')\n",
    "\n",
    "var_encoder.to(device)\n",
    "decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3362a68c",
   "metadata": {
    "id": "3362a68c"
   },
   "outputs": [],
   "source": [
    "def gaussian_likelihood(x_hat, logscale, x):\n",
    "    scale = torch.exp(logscale)\n",
    "    mean = x_hat\n",
    "    dist = torch.distributions.Normal(mean, scale)\n",
    "\n",
    "    # measure prob of seeing image under p(x|z)\n",
    "    likelihood = dist.log_prob(x)\n",
    "    return likelihood.sum(dim=(1, 2, 3))\n",
    "\n",
    "def KL_divergence(z, mu, std):\n",
    "    #### Compute the KL divergence\n",
    "\n",
    "    # 1. define the first two probabilities (in this case Normal for both)\n",
    "    p = torch.distributions.Normal(torch.zeros_like(mu), torch.ones_like(std)) \n",
    "    # we force the distribution to a standard normal distribution\n",
    "    q = torch.distributions.Normal(mu, std)\n",
    "\n",
    "    # 2. get the probabilities from the two distributions\n",
    "\n",
    "    log_qzx = q.log_prob(z) \n",
    "    log_pz = p.log_prob(z)\n",
    "\n",
    "    # 3. compute the kl divergence\n",
    "\n",
    "    kl = (log_qzx - log_pz)\n",
    "    kl = kl.sum(-1)\n",
    "    return kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59a3101",
   "metadata": {
    "id": "d59a3101"
   },
   "outputs": [],
   "source": [
    "### Training function\n",
    "def train_epoch(var_encoder, decoder, device, dataloader, optimizer):\n",
    "    # Set train mode for both the encoder and the decoder\n",
    "    var_encoder.train()\n",
    "    decoder.train()\n",
    "    train_loss = []\n",
    "    # Iterate the dataloader (we do not need the label values, this is unsupervised learning)\n",
    "    for x, _ in dataloader: \n",
    "        # Move tensor to the proper device\n",
    "        x = x.to(device)\n",
    "        # Get the mean and std vectors from the encoder\n",
    "        [mean, log_var] = var_encoder(x)\n",
    "        # Sample the distribution to obtain a differentiable function\n",
    "        std = torch.exp(log_var / 2) # this is needed to ensure that the std is positive\n",
    "        q = torch.distributions.Normal(mean, std)\n",
    "        z = q.rsample()\n",
    "        # Obtain the output of the encoder\n",
    "        x_hat = decoder(z)\n",
    "        \n",
    "        # Evaluate reconstruction loss + KL divergence\n",
    "        recon_loss = gaussian_likelihood(x_hat, var_encoder.log_scale, x)\n",
    "        kl = KL_divergence(z, mean, std)\n",
    "        loss = (-recon_loss + kl).mean()\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.item())\n",
    "    print('\\t train loss: %f' % (np.mean(train_loss)))\n",
    "    return np.mean(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a783a9",
   "metadata": {
    "id": "f7a783a9"
   },
   "outputs": [],
   "source": [
    "### Evaluation function\n",
    "\n",
    "def test_epoch(var_encoder, decoder, device, dataloader):\n",
    "    # Set evaluation mode\n",
    "    var_encoder.eval()\n",
    "    decoder.eval()\n",
    "    val_loss = []\n",
    "    with torch.no_grad():\n",
    "        for x, _ in dataloader:\n",
    "            x = x.to(device)\n",
    "            \n",
    "            # Get the mean and std vectors from the encoder\n",
    "            [mean, log_var] = var_encoder(x)\n",
    "            \n",
    "            # Sample the distribution to obtain a differentiable function\n",
    "            std = torch.exp(log_var / 2) # this is needed to ensure that the std is positive\n",
    "            q = torch.distributions.Normal(mean, std)\n",
    "            z = q.rsample()\n",
    "            \n",
    "            # Obtain the output of the encoder\n",
    "            x_hat = decoder(z)\n",
    "\n",
    "            # Evaluate reconstruction loss + KL divergence\n",
    "            recon_loss = gaussian_likelihood(x_hat, var_encoder.log_scale, x)\n",
    "            kl = KL_divergence(z, mean, std)\n",
    "            loss = (-recon_loss + kl).mean()\n",
    "            \n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "    return np.mean(val_loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8d2530",
   "metadata": {
    "id": "6f8d2530"
   },
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b3bf28",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "09b3bf28",
    "outputId": "8b30d544-863a-467b-8894-3d6e79eda9b5"
   },
   "outputs": [],
   "source": [
    "if not trained:\n",
    "    ### Training cycle\n",
    "    patience = 3\n",
    "    best = np.infty\n",
    "    num_epochs = 100\n",
    "    for epoch in range(num_epochs):\n",
    "        print('EPOCH %d/%d' % (epoch + 1, num_epochs))\n",
    "        ### Training (use the training function)\n",
    "        train_epoch(\n",
    "            var_encoder=var_encoder, \n",
    "            decoder=decoder, \n",
    "            device=device, \n",
    "            dataloader=train_dataloader,  \n",
    "            optimizer=optimizer)\n",
    "        ### Validation  (use the testing function)\n",
    "        val_loss = test_epoch(\n",
    "            var_encoder=var_encoder, \n",
    "            decoder=decoder, \n",
    "            device=device, \n",
    "            dataloader=test_dataloader)\n",
    "        # Print Validationloss\n",
    "        print('\\n\\n\\t VALIDATION - EPOCH %d/%d - loss: %f\\n\\n' % (epoch + 1, num_epochs, val_loss))\n",
    "        if val_loss < best:\n",
    "            best = val_loss\n",
    "            patience = 3\n",
    "        else:\n",
    "            patience -= 1\n",
    "        if patience == 0:\n",
    "            print(\"Learning stopped\")\n",
    "            break\n",
    "        ### Plot progress\n",
    "        # Get the output of a specific image (the test image at index 0 in this case)\n",
    "        img = test_dataset[0][0].unsqueeze(0).to(device)\n",
    "        var_encoder.eval()\n",
    "        decoder.eval()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            [mean, log_var] = var_encoder(img)\n",
    "            std = torch.exp(log_var / 2) # this is needed to ensure that the std is positive\n",
    "            q = torch.distributions.Normal(mean, std)\n",
    "            z = q.rsample()\n",
    "            rec_img  = decoder(z)\n",
    "\n",
    "        # Plot the reconstructed image\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(12,6))\n",
    "        axs[0].imshow(img.cpu().squeeze().numpy(), cmap='gist_gray')\n",
    "        axs[0].set_title('Original image')\n",
    "        axs[1].imshow(rec_img.cpu().squeeze().numpy(), cmap='gist_gray')\n",
    "        axs[1].set_title('Reconstructed image (EPOCH %d)' % (epoch + 1))\n",
    "        plt.tight_layout()\n",
    "        plt.pause(0.1)\n",
    "        # Save figures\n",
    "        os.makedirs('autoencoder_progress_%d_features' % d, exist_ok=True)\n",
    "        #fig.savefig('autoencoder_progress_%d_features/epoch_%d.jpg' % (d, epoch + 1))\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        # Save network parameters\n",
    "        torch.save(var_encoder.state_dict(), 'var_encoder_params.pth')\n",
    "        torch.save(decoder.state_dict(), 'var_decoder_params.pth')\n",
    "else:\n",
    "    # Load network parameters\n",
    "    \n",
    "    var_encoder.load_state_dict(torch.load('var_encoder_params.pth', map_location=device))\n",
    "    decoder.load_state_dict(torch.load('var_decoder_params.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8865470",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some examples of reconstructed images\n",
    "indices = np.random.randint(len(test_dataset), size=8)\n",
    "subset = torch.utils.data.Subset(test_dataset, indices)\n",
    "testloader_subset = DataLoader(subset, batch_size=1, num_workers=0, shuffle=False)\n",
    "    \n",
    "var_encoder.eval()\n",
    "decoder.eval()\n",
    "fig, axs = plt.subplots(2, 4, figsize=(15,15))\n",
    "axs = axs.flatten()\n",
    "ax_n = 0\n",
    "## Iterate trough the samples in the test dataset\n",
    "iterator = iter(testloader_subset)\n",
    "loop = True\n",
    "while loop:\n",
    "    try:\n",
    "        data, label = next(iterator)\n",
    "    except StopIteration:\n",
    "        loop = False\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            # In this case we need to sample from a normal distribution \n",
    "            [mean, log_var] = var_encoder(data)\n",
    "            std = torch.exp(log_var / 2) # this is needed to ensure that the std is positive\n",
    "            q = torch.distributions.Normal(mean, std)\n",
    "            z = q.rsample()\n",
    "            out  = decoder(z)\n",
    "            \n",
    "        axs[ax_n].set_xticks([])\n",
    "        axs[ax_n].set_yticks([])\n",
    "        axs[ax_n].imshow(out.cpu().squeeze().numpy(), cmap='gist_gray')\n",
    "        axs[ax_n].set_title(label_names[label[0].item()], fontsize = 14)\n",
    "        ax_n += 1\n",
    "        plt.tight_layout()\n",
    "#plt.savefig(\"Examples_variational.pdf\", format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5622ae07",
   "metadata": {},
   "source": [
    "### Latent space analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3420c86",
   "metadata": {
    "id": "f3420c86"
   },
   "outputs": [],
   "source": [
    "encoded_samples = []\n",
    "for sample in tqdm(test_dataset):\n",
    "    img = sample[0].unsqueeze(0).to(device)\n",
    "    label = sample[1]\n",
    "    # Encode image\n",
    "    var_encoder.eval()\n",
    "    with torch.no_grad():\n",
    "        encoded_img, _  = var_encoder(img)\n",
    "    # Append to list\n",
    "    encoded_img = encoded_img.flatten().cpu().numpy()\n",
    "    encoded_sample = {f\"Enc. Variable {i}\": enc for i, enc in enumerate(encoded_img)}\n",
    "    encoded_sample['label'] = label\n",
    "    encoded_samples.append(encoded_sample)\n",
    "\n",
    "encoded_samples = pd.DataFrame(encoded_samples)\n",
    "encoded_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dce5a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2).fit(encoded_samples[:-2])\n",
    "encoded_samples_pca = pca.transform(encoded_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b69f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(encoded_samples_pca, x=0, y=1,\n",
    "                 color=encoded_samples.label.astype(str),\n",
    "                 labels={'0': 'Feature 1', '1': 'Feature 2', 'Color': 'Class'})\n",
    "\n",
    "fig.for_each_trace(lambda t: t.update(name = dictionary[t.name]))\n",
    "#fig.write_image(\"PCA_variational.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cf1d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the pca find two internal representation\n",
    "coat = np.array([0.5, 1.3]) \n",
    "boot = np.array([-5.5, -1])\n",
    "sample1 = torch.tensor(np.dot(coat, pca.components_) + pca.mean_)[:-1]\n",
    "sample2 = torch.tensor(np.dot(boot, pca.components_) + pca.mean_)[:-1]\n",
    "samples = [sample1, sample2]\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "encoder.float()\n",
    "decoder.float()\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15,15))\n",
    "axs = axs.flatten()\n",
    "ax_n = 0\n",
    "while ax_n < 2:\n",
    "    with torch.no_grad():\n",
    "        out = decoder(samples[ax_n].float().unsqueeze(0).to(device)) # add the batch dimension\n",
    "    axs[ax_n].set_xticks([])\n",
    "    axs[ax_n].set_yticks([])\n",
    "    axs[ax_n].imshow(out.cpu().squeeze().numpy(), cmap='gist_gray')\n",
    "    ax_n += 1\n",
    "    plt.tight_layout()\n",
    "#plt.savefig(\"new_examples_variational.pdf\", format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c856b73a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e491b47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "homework2_v3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "02187d9ccaa5436e91d26aabff2f7331": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "075ff3fbfa9e4bb690e1274bdef7d440": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9f09f15ae7db41d0885ea9caeab875c6",
      "placeholder": "​",
      "style": "IPY_MODEL_1f49bf7d056f40da9058485e9d85b677",
      "value": " 6144/? [00:00&lt;00:00, 98111.63it/s]"
     }
    },
    "0b535b199f5d49ed861efe5b3d181bb2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0c4533dfc7ea42f98337447a9970fe01": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f9f0710f7bbf45bb973d398edad0c5a6",
       "IPY_MODEL_4ba2ac83b4164aa5b19d6780531e6c92",
       "IPY_MODEL_c572494a9e8c482a9e0b87990e8e24d2"
      ],
      "layout": "IPY_MODEL_fd3cc333c2cf4230bae890387c2dd003"
     }
    },
    "0c7c8a6976d24c98b34b28781050b976": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0e54025c37444b8198a77d7be5236415": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_56cddf80f3c4419e997d8e2914b4bd57",
      "placeholder": "​",
      "style": "IPY_MODEL_3e762c4342c84b7b9b3122f09adc3e70",
      "value": ""
     }
    },
    "1022c1848e6a404dbd4f0dd6251d417d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1f49bf7d056f40da9058485e9d85b677": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2679dfab50d24d7da95fab0d5510d3a5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2696f56eecab4d22a9f2375638996d51": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a90c98c4f2004de3972eeb8cb3c37959",
      "placeholder": "​",
      "style": "IPY_MODEL_73dc231b525144c983002ef511b3657a",
      "value": " 29696/? [00:00&lt;00:00, 124397.78it/s]"
     }
    },
    "2b3bcc308f3f4d5eb4548bd98d2b9f19": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2e274f81dce14236a25c0c60ed84d7dc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3503e32bc3304c4db6e6623cb54a6bf1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3a26c161c4494eee9af30ba08e5319dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3e762c4342c84b7b9b3122f09adc3e70": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4ba2ac83b4164aa5b19d6780531e6c92": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a1c8d41c6ce44e1cb2883bef5d81a319",
      "max": 26421880,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9f18dd4dac38468093f9eba4d98717e9",
      "value": 26421880
     }
    },
    "56cddf80f3c4419e997d8e2914b4bd57": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5ff0b1e962334e35b1dc5af8ab929121": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6389d9a3c9b3486eba066e188ea847e8",
      "max": 29515,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c19d3c06a1dc4632ad9291f7824ceb54",
      "value": 29515
     }
    },
    "609ca3add2a449008d492c8f6df622e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0e54025c37444b8198a77d7be5236415",
       "IPY_MODEL_845d3bf4de304446a82bd5803d287f38",
       "IPY_MODEL_075ff3fbfa9e4bb690e1274bdef7d440"
      ],
      "layout": "IPY_MODEL_779c56e2affe428faa1be4db4dbdc4ed"
     }
    },
    "6389d9a3c9b3486eba066e188ea847e8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "73dc231b525144c983002ef511b3657a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "779c56e2affe428faa1be4db4dbdc4ed": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7f69ec1c0c134003b3b3f2dfe61cb804": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "845d3bf4de304446a82bd5803d287f38": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2e274f81dce14236a25c0c60ed84d7dc",
      "max": 5148,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3a26c161c4494eee9af30ba08e5319dd",
      "value": 5148
     }
    },
    "936ab4d04e3940f0a0a20387af830ef1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fc2393a9331b4243bc8b85ed0693b66b",
       "IPY_MODEL_5ff0b1e962334e35b1dc5af8ab929121",
       "IPY_MODEL_2696f56eecab4d22a9f2375638996d51"
      ],
      "layout": "IPY_MODEL_7f69ec1c0c134003b3b3f2dfe61cb804"
     }
    },
    "9f09f15ae7db41d0885ea9caeab875c6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9f18dd4dac38468093f9eba4d98717e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9fc8fcf86d72424188c5883c0c77009f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d88c41fbc9654084a253b6618c8cab2b",
       "IPY_MODEL_c1d5274e75f24356b3dd4beb0f7417bd",
       "IPY_MODEL_ec264e85179f4107bfc4babd064770c6"
      ],
      "layout": "IPY_MODEL_affcab26790d4b969a3f6a59d0c8a2f4"
     }
    },
    "a1c8d41c6ce44e1cb2883bef5d81a319": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a90c98c4f2004de3972eeb8cb3c37959": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "affcab26790d4b969a3f6a59d0c8a2f4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c19d3c06a1dc4632ad9291f7824ceb54": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c1d5274e75f24356b3dd4beb0f7417bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2b3bcc308f3f4d5eb4548bd98d2b9f19",
      "max": 4422102,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e5db4b9598c248a6b3036a12c899f203",
      "value": 4422102
     }
    },
    "c3f55e96d8a14f2887a370d94b48aa6b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c572494a9e8c482a9e0b87990e8e24d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2679dfab50d24d7da95fab0d5510d3a5",
      "placeholder": "​",
      "style": "IPY_MODEL_02187d9ccaa5436e91d26aabff2f7331",
      "value": " 26422272/? [00:01&lt;00:00, 22925048.22it/s]"
     }
    },
    "cfd333b52a554a2b83e1b0a16f79bd95": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d88c41fbc9654084a253b6618c8cab2b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1022c1848e6a404dbd4f0dd6251d417d",
      "placeholder": "​",
      "style": "IPY_MODEL_0b535b199f5d49ed861efe5b3d181bb2",
      "value": ""
     }
    },
    "e0e039d792f94b428f50e9257c4da45d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e5db4b9598c248a6b3036a12c899f203": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ec264e85179f4107bfc4babd064770c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3503e32bc3304c4db6e6623cb54a6bf1",
      "placeholder": "​",
      "style": "IPY_MODEL_0c7c8a6976d24c98b34b28781050b976",
      "value": " 4422656/? [00:00&lt;00:00, 8766813.55it/s]"
     }
    },
    "f745645620394813896ac7bf849c0b8c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f9f0710f7bbf45bb973d398edad0c5a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c3f55e96d8a14f2887a370d94b48aa6b",
      "placeholder": "​",
      "style": "IPY_MODEL_e0e039d792f94b428f50e9257c4da45d",
      "value": ""
     }
    },
    "fc2393a9331b4243bc8b85ed0693b66b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cfd333b52a554a2b83e1b0a16f79bd95",
      "placeholder": "​",
      "style": "IPY_MODEL_f745645620394813896ac7bf849c0b8c",
      "value": ""
     }
    },
    "fd3cc333c2cf4230bae890387c2dd003": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
