{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yfo0paMoKbTf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler, random_split\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import KFold "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "STvvU5CKKvm2"
   },
   "source": [
    "* The goal is to train a neural network to approximate an unknown function:\n",
    "$$ \n",
    "f:\\mathbb{R}→\\mathbb{R} \\\\\n",
    "x↦y=f(x) \\\\\n",
    "\\text{network}(x) \\approx f(x)\n",
    "$$\n",
    "* As training point, you only have noisy measures from the target function.\n",
    "$$\n",
    "\\hat{y} = f(x) + noise\n",
    "$$\n",
    "* Consider to create a validation set from you training data, or use a k-fold cross-validation strategy. You may find useful these functions from the `scikit-learn` library:\n",
    "    - [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "    - [KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jc6zyNmKLCUE",
    "outputId": "82c9edd0-7cb6-4e1f-f70a-5bb5b388acf5"
   },
   "outputs": [],
   "source": [
    "!wget -P regression_dataset https://gitlab.dei.unipd.it/michieli/nnld-2021-22-lab-resources/-/raw/main/homework1/train_data.csv\n",
    "!wget -P regression_dataset https://gitlab.dei.unipd.it/michieli/nnld-2021-22-lab-resources/-/raw/main/homework1/test_data.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IUEZK_xaLKvO"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained = True\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BIr599r6LZFk"
   },
   "source": [
    "### Network Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "di5dgCJ7Eygg"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, Ni, Nh1, Nh2, No):\n",
    "        \"\"\"\n",
    "        Ni - Input size\n",
    "        Nh1 - Neurons in the 1st hidden layer\n",
    "        Nh2 - Neurons in the 2nd hidden layer\n",
    "        No - Output size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features = Ni, out_features = Nh1)\n",
    "        self.fc2 = nn.Linear(in_features = Nh1, out_features = Nh2)\n",
    "        self.out = nn.Linear(in_features=Nh2, out_features=No)\n",
    "        self.act = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, additional_out=False):\n",
    "        x = self.act(self.fc1(x))\n",
    "        x = self.act(self.fc2(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fw1w08atLl84"
   },
   "outputs": [],
   "source": [
    "def train_step(model, train_loader, loss_fn, optimizer, printer=True):\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    for sample_batched in train_loader:\n",
    "        \n",
    "        # Move data to device\n",
    "        x_batch = sample_batched[0].to(device)\n",
    "        label_batch = sample_batched[1].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        out = model(x_batch)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(out, label_batch)\n",
    "\n",
    "        # Backpropagation\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Save train loss for this batch\n",
    "        loss_batch = loss.detach().cpu().numpy()\n",
    "        train_loss.append(loss_batch)  \n",
    "    # Save average train loss over the batches\n",
    "    train_loss = np.mean(train_loss)\n",
    "    if(printer): print(f\"AVERAGE TRAIN LOSS: {train_loss}\")\n",
    "    return train_loss\n",
    "\n",
    "    \n",
    "def validation_step(model, val_loader, loss_fn, printer=True):\n",
    "    val_loss = []\n",
    "    \n",
    "    ### VALIDATION\n",
    "    model.eval() # Evaluation mode (e.g. disable dropout, batchnorm,...)\n",
    "    with torch.no_grad(): # Disable gradient tracking\n",
    "        for sample_batched in val_loader:\n",
    "            # Move data to device\n",
    "            x_batch = sample_batched[0].to(device)\n",
    "            label_batch = sample_batched[1].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            out = model(x_batch)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(out, label_batch)\n",
    "\n",
    "            # Save val loss for this batch\n",
    "            loss_batch = loss.detach().cpu().numpy()\n",
    "            val_loss.append(loss_batch)\n",
    "\n",
    "    # Save average validation loss\n",
    "    val_loss = np.mean(val_loss)\n",
    "    if(printer): print(f\"AVERAGE VAL LOSS: {val_loss}\")\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qNPmkogE7e40"
   },
   "outputs": [],
   "source": [
    "# Load from pandas\n",
    "\n",
    "train_df = pd.read_csv('regression_dataset/train_data.csv')\n",
    "test_df = pd.read_csv('regression_dataset/test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XDeZl3EW8mez"
   },
   "outputs": [],
   "source": [
    "# DataSet class \n",
    "\n",
    "class CSVDataset(Dataset):\n",
    "\n",
    "    def __init__(self, file, transform=None):\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        for element in file.index:\n",
    "            inp = file.iloc[element]['input']\n",
    "            label = file.iloc[element]['label']\n",
    "            self.data.append((float(inp), float(label)))\n",
    "    # Now self.data contains all our dataset.\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Our sample is the element idx of the list self.data        \n",
    "        sample = self.data[idx]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        # The length of the dataset is simply the length of the self.data list\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IKmbWwxh-CMy"
   },
   "outputs": [],
   "source": [
    "class toTensor(object):\n",
    "    def __call__(self, sample):\n",
    "        x,y = sample\n",
    "        return (torch.tensor([x]).float(), torch.tensor([y]).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GwJ8n-z0_kp1"
   },
   "outputs": [],
   "source": [
    "composed_transform = transforms.Compose([toTensor()])\n",
    "\n",
    "train_dataset = CSVDataset(train_df, transform=composed_transform)\n",
    "test_dataset = CSVDataset(test_df, transform=composed_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ax_wspDH9Xa",
    "outputId": "646f37f2-8e5a-4a53-e494-f143a691d0ba"
   },
   "outputs": [],
   "source": [
    "# Check if the GPU is available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Training device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oAhKZHn2Eygk"
   },
   "source": [
    "### DataLoaders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gH3D-0e4Eygk"
   },
   "outputs": [],
   "source": [
    "# Train-test splitting for dataset\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset_smpl, val_dataset_smpl = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# Dataloaders\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset_smpl, batch_size=int(len(train_dataset_smpl)/2), shuffle=True, num_workers=0)\n",
    "val_dataloader = DataLoader(val_dataset_smpl, batch_size=len(val_dataset_smpl), shuffle=False, num_workers=0)\n",
    "\n",
    "test_dataloader  = DataLoader(test_dataset,  batch_size=len(test_dataset), shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WjdsigQCEygl"
   },
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "loss_fn = nn.HuberLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MEw5kXcMMUtP",
    "outputId": "85f2c531-4b9b-431f-f676-2cee68106743"
   },
   "outputs": [],
   "source": [
    "#### TRAINING LOOP\n",
    "\n",
    "net = Net(1, 256, 128, 1)\n",
    "# TRAIN!\n",
    "num_epochs = 500\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=1e-3)\n",
    "\n",
    "net.to(device)\n",
    "train_loss_log = []\n",
    "val_loss_log = []\n",
    "for epoch in range(num_epochs):\n",
    "    print('#################')\n",
    "    print(f'# EPOCH {epoch}')\n",
    "    print('#################')\n",
    "    t_loss = train_step(net, train_dataloader, loss_fn, optimizer)\n",
    "    train_loss_log.append(t_loss)\n",
    "    v_loss = validation_step(net, val_dataloader, loss_fn)\n",
    "    val_loss_log.append(v_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 499
    },
    "id": "mJdqcz-yEygl",
    "outputId": "b9b874ec-3201-4099-a784-a9a8cdf32100"
   },
   "outputs": [],
   "source": [
    "# Plot losses\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.semilogy(train_loss_log, label='Train loss')\n",
    "plt.semilogy(val_loss_log, label='Validation loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "#plt.savefig('simple_losses.pdf', format = 'pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8sNN7DSHEygm"
   },
   "source": [
    "### Access network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CRIuM3llEygm"
   },
   "outputs": [],
   "source": [
    "# First hidden layer\n",
    "h1_w = net.fc1.weight.data.cpu().numpy()\n",
    "h1_b = net.fc1.bias.data.cpu().numpy()\n",
    "\n",
    "# Second hidden layer\n",
    "h2_w = net.fc2.weight.data.cpu().numpy()\n",
    "h2_b = net.fc2.bias.data.cpu().numpy()\n",
    "\n",
    "# Output layer\n",
    "out_w = net.out.weight.data.cpu().numpy()\n",
    "out_b = net.out.bias.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3EEqWWEwEygm"
   },
   "source": [
    "### Weights histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 585
    },
    "id": "YHvNrmy6Eygm",
    "outputId": "21a77c1b-16b9-44b6-c8de-cbdfd8a1aab6"
   },
   "outputs": [],
   "source": [
    "# Weights histogram\n",
    "fig, axs = plt.subplots(3, 1, figsize=(12,8))\n",
    "axs[0].hist(h1_w.flatten(), 50)\n",
    "axs[0].set_title('First hidden layer weights')\n",
    "axs[1].hist(h2_w.flatten(), 50)\n",
    "axs[1].set_title('Second hidden layer weights')\n",
    "axs[2].hist(out_w.flatten(), 50)\n",
    "axs[2].set_title('Output layer weights')\n",
    "[ax.grid() for ax in axs]\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ciqk7PE4G1yE"
   },
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 552
    },
    "id": "oZVOkLIkF_Lr",
    "outputId": "e55c9c9e-ee42-4517-b574-fc9461992c6c"
   },
   "outputs": [],
   "source": [
    "for sample_batched in test_dataloader:\n",
    "    # Move data to device\n",
    "    x_batch = sample_batched[0].to(device)\n",
    "    label_batch = sample_batched[1].to(device)\n",
    "\n",
    "    net.eval()\n",
    "    with torch.no_grad(): # turn off gradients computation\n",
    "        y_vec = net(x_batch)\n",
    "        error = nn.functional.mse_loss(y_vec, label_batch) #Compute the mean squared error\n",
    "        \n",
    "        y_vec = y_vec.cpu().numpy()\n",
    "        label_batch = label_batch.cpu().numpy()\n",
    "        # Plot output\n",
    "        plt.figure(figsize=(12,8))\n",
    "        plt.plot(y_vec, label='Network output')\n",
    "        plt.plot(label_batch, label='True model')\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('y')\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        #plt.savefig('simple_prediction.pdf', format = 'pdf', bbox_inches = 'tight')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "print('#################')\n",
    "print(f'# MSE {error}')\n",
    "print('#################')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ZzKoVdOEygn"
   },
   "source": [
    "### Advanced desing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AXlBinuKEygo"
   },
   "outputs": [],
   "source": [
    "class Net_v2(nn.Module):\n",
    "    \n",
    "    def __init__(self, Ni, Nh1, Nh2, No):\n",
    "        \"\"\"\n",
    "        Ni - Input size\n",
    "        Nh1 - Neurons in the 1st hidden layer\n",
    "        Nh2 - Neurons in the 2nd hidden layer\n",
    "        No - Output size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features = Ni, out_features = Nh1)\n",
    "        self.fc2 = nn.Linear(in_features = Nh1, out_features = Nh2)\n",
    "        self.out = nn.Linear(in_features=Nh2, out_features=No)\n",
    "        self.act = nn.ELU() \n",
    "        print(\"Network initialized!\")\n",
    "        \n",
    "    def forward(self, x, additional_out=False):\n",
    "        x = self.act(self.fc1(x))\n",
    "        x = self.act(self.fc2(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xGyUc1huEygp",
    "outputId": "7502ce3d-06b1-40ad-d367-4841348ddb47"
   },
   "outputs": [],
   "source": [
    "if not trained:\n",
    "    ###########OPTIMIZATION###########\n",
    "    ! pip install optuna\n",
    "    import optuna\n",
    "    from optuna.integration import PyTorchLightningPruningCallback\n",
    "    EPOCHS = 250\n",
    "\n",
    "    train_loss_log = []\n",
    "    val_loss_log = []\n",
    "\n",
    "    def objective(trial):\n",
    "\n",
    "        # Number of units for hidden layer\n",
    "        output_dims = [trial.suggest_int(\"n_units_l{}\".format(i), 64, 256, log=True) for i in range(2)]\n",
    "        # Learning rate (log)\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n",
    "        # Weight decay \n",
    "        w_decay = trial.suggest_float(\"weight_decay\", 0, 1e-7)\n",
    "\n",
    "        # Delta of loss (threshold to change between delta-scaled L1 and L2 loss)\n",
    "        delta = trial.suggest_float(\"delta\", 0, 2)\n",
    "\n",
    "        model = Net_v2(1, output_dims[0], output_dims[1], 1).to(device)\n",
    "        optimizer_name = \"Adam\"\n",
    "        optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=learning_rate, weight_decay=w_decay)\n",
    "\n",
    "        loss_func = nn.HuberLoss(delta=delta)\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            t_loss = train_step(model, train_dataloader, loss_fn, optimizer, printer = False)\n",
    "            train_loss_log.append(t_loss)\n",
    "            v_loss = validation_step(model, val_dataloader, loss_fn, printer = False)\n",
    "            val_loss_log.append(v_loss)\n",
    "\n",
    "            trial.report(val_loss_log[-1], epoch)\n",
    "\n",
    "            # Handle pruning based on the intermediate value.\n",
    "            if trial.should_prune():\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        return val_loss_log[-1]\n",
    "\n",
    "    pruner: optuna.pruners.BasePruner = optuna.pruners.MedianPruner(n_startup_trials=3, n_warmup_steps=10, interval_steps=1)\n",
    "\n",
    "    study = optuna.create_study(study_name=\"Regression\", direction=\"minimize\", pruner=pruner)\n",
    "    study.optimize(objective, n_trials=1000)\n",
    "\n",
    "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iiPSX9WjIQUJ",
    "outputId": "fd21260f-c050-4bcf-9750-fa6300a6ec8c"
   },
   "outputs": [],
   "source": [
    "print('########## BEST TRIAL ########## \\n')\n",
    "print('Params:\\n')\n",
    "print(\"Number of units first hidden layer: 163\")\n",
    "print(\"Number of units second hidden layer: 150\")\n",
    "print('learning rate: 0.008868384223793775')\n",
    "print('weight decay: 4.896378094756895e-08')\n",
    "print('delta: 1.0377865474636057')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YwiHMYN9Eygo",
    "outputId": "92eee5f0-e299-465e-b74e-de255a0b7d72"
   },
   "outputs": [],
   "source": [
    "net2 = Net_v2(1, 163, 150, 1)\n",
    "net2.to(device)\n",
    "optimizer = optim.Adam(net2.parameters(), lr=0.008868384223793775, weight_decay=4.896378094756895e-08)\n",
    "loss_fn = nn.HuberLoss(delta=1.0377865474636057)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8XfLkwyEygo"
   },
   "source": [
    "### Network weights before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 585
    },
    "id": "71B8KTSWEygo",
    "outputId": "b93ddcae-0156-42f5-b797-0d6f6391daf3"
   },
   "outputs": [],
   "source": [
    "\n",
    "h1_w = net2.fc1.weight.data.cpu().numpy()\n",
    "h1_b = net2.fc1.bias.data.cpu().numpy()\n",
    "\n",
    "h2_w = net2.fc2.weight.data.cpu().numpy()\n",
    "h2_b = net2.fc2.bias.data.cpu().numpy()\n",
    "\n",
    "out_w = net2.out.weight.data.cpu().numpy()\n",
    "out_b = net2.out.bias.data.cpu().numpy()\n",
    "\n",
    "# Weights histogram\n",
    "fig, axs = plt.subplots(3, 1, figsize=(12,8))\n",
    "axs[0].hist(h1_w.flatten(), 50)\n",
    "axs[0].set_title('First hidden layer weights', fontsize = 18)\n",
    "axs[1].hist(h2_w.flatten(), 50)\n",
    "axs[1].set_title('Second hidden layer weights', fontsize = 18)\n",
    "axs[2].hist(out_w.flatten(), 50)\n",
    "axs[2].set_title('Output layer weights', fontsize = 18)\n",
    "[ax.grid() for ax in axs]\n",
    "plt.tight_layout()\n",
    "#plt.savefig('Weights_before.pdf', format = 'pdf', bbox_inches = 'tight') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## KFold split \n",
    "k=10\n",
    "splits=KFold(n_splits=k,shuffle=True,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZnonIZbOEygp",
    "outputId": "4477d55a-6255-4b52-f054-26f7a88d64b5"
   },
   "outputs": [],
   "source": [
    "if not trained:\n",
    "    ######## TRAIN ########\n",
    "    train_loss_log = []\n",
    "    val_loss_log = []\n",
    "    best_loss = np.infty\n",
    "    num_epochs = 300\n",
    "    patience = 15\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('#################')\n",
    "        print(f'# EPOCH {epoch}')\n",
    "        print('#################')\n",
    "\n",
    "        train_fold = []\n",
    "        test_fold = []\n",
    "\n",
    "        # Train the model for each fold\n",
    "        for train_idx,val_idx in splits.split(train_dataset):\n",
    "\n",
    "            train_sampler = SubsetRandomSampler(train_idx)\n",
    "            test_sampler = SubsetRandomSampler(val_idx)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=len(train_sampler), sampler=train_sampler)\n",
    "            test_loader = DataLoader(train_dataset, batch_size=len(test_sampler), sampler=test_sampler)\n",
    "\n",
    "            t_loss = train_step(net2, train_loader, loss_fn, optimizer, printer=False)\n",
    "            v_loss = validation_step(net2, test_loader, loss_fn, printer=False)\n",
    "\n",
    "            train_fold.append(t_loss)\n",
    "            test_fold.append(v_loss)\n",
    "        # Average the performances of the fold\n",
    "        train_loss_log.append(np.mean(train_fold))\n",
    "        print(f\"AVERAGE TRAIN LOSS: {train_loss_log[-1]}\")\n",
    "        val_loss_log.append(np.mean(test_fold))\n",
    "        print(f\"AVERAGE VAL LOSS: {val_loss_log[-1]}\")\n",
    "\n",
    "        # Implement early stopping\n",
    "        if(val_loss_log[-1] < best_loss):\n",
    "            best_loss = val_loss_log[-1]\n",
    "            patience = 15\n",
    "        else:\n",
    "            patience -= 1\n",
    "            if(patience == 0): \n",
    "                print(\"#################\\nLearning stopped because the validation error was not improving\\n#################\")\n",
    "                break \n",
    "                \n",
    "    #save the model               \n",
    "    net_state_dict = net2.state_dict()\n",
    "    print(net_state_dict.keys())\n",
    "    # Save the state dict to a file\n",
    "    torch.save(net_state_dict, 'regressor_v3.torch')\n",
    "else:\n",
    "    # Load the state dict previously saved\n",
    "    net_state_dict = torch.load('regressor_v3.torch')\n",
    "    # Update the network parameters\n",
    "    net2.load_state_dict(net_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "id": "pL829lWbORL3",
    "outputId": "d1d2dd85-52e4-4a20-9d5f-0392d3622ad2"
   },
   "outputs": [],
   "source": [
    "if not trained:\n",
    "    # Plot losses\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.semilogy(train_loss_log, label='Train loss')\n",
    "    plt.semilogy(val_loss_log, label='Validation loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    #plt.savefig('loss_regression.pdf', format = 'pdf', bbox_inches = 'tight') \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZV6aqgm6Eygp"
   },
   "source": [
    "### Network weights after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 585
    },
    "id": "KvqanNwZEygp",
    "outputId": "90ad85ae-9dce-47e0-bb4b-8cfe2ff23989"
   },
   "outputs": [],
   "source": [
    "h1_w = net2.fc1.weight.data.cpu().numpy()\n",
    "h1_b = net2.fc1.bias.data.cpu().numpy()\n",
    "\n",
    "h2_w = net2.fc2.weight.data.cpu().numpy()\n",
    "h2_b = net2.fc2.bias.data.cpu().numpy()\n",
    "\n",
    "out_w = net2.out.weight.data.cpu().numpy()\n",
    "out_b = net2.out.bias.data.cpu().numpy()\n",
    "\n",
    "# Weights histogram\n",
    "fig, axs = plt.subplots(3, 1, figsize=(12,8))\n",
    "axs[0].hist(h1_w.flatten(), 50)\n",
    "axs[0].set_title('First hidden layer weights', fontsize = 18)\n",
    "axs[1].hist(h2_w.flatten(), 50)\n",
    "axs[1].set_title('Second hidden layer weights', fontsize = 18)\n",
    "axs[2].hist(out_w.flatten(), 50)\n",
    "axs[2].set_title('Output layer weights', fontsize = 18)\n",
    "[ax.grid() for ax in axs]\n",
    "plt.tight_layout()\n",
    "#plt.savefig('weights_after_regression.pdf', format = 'pdf', bbox_inches = 'tight') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation(layer, input, output):\n",
    "    global activation\n",
    "    activation = torch.nn.functional.elu(output)\n",
    "\n",
    "### Register hook  \n",
    "hook_handle_1 = net2.fc1.register_forward_hook(get_activation)\n",
    "\n",
    "### Analyze activations\n",
    "net2 = net2.to(device)\n",
    "net2.eval()\n",
    "with torch.no_grad():\n",
    "    x1 = torch.tensor([10]).float().to(device)\n",
    "    y1 = net2(x1)\n",
    "    z1 = activation\n",
    "    x2 = torch.tensor([-100]).float().to(device)\n",
    "    y2 = net2(x2)\n",
    "    z2 = activation\n",
    "    x3 = torch.tensor([200]).float().to(device)\n",
    "    y3 = net2(x3)\n",
    "    z3 = activation\n",
    "\n",
    "### Remove hook\n",
    "hook_handle_1.remove()\n",
    "\n",
    "### Plot activations\n",
    "fig, axs = plt.subplots(3, 1, figsize=(12,6))\n",
    "axs[0].stem(z1.cpu().numpy(), use_line_collection=True)\n",
    "axs[0].set_title('First layer activations for input x=%.2f' % x1, fontsize = 18)\n",
    "axs[1].stem(z2.cpu().numpy(), use_line_collection=True)\n",
    "axs[1].set_title('First layer activations for input x=%.2f' % x2, fontsize = 18)\n",
    "axs[2].stem(z3.cpu().numpy(), use_line_collection=True)\n",
    "axs[2].set_title('First layer activations for input x=%.2f' % x3, fontsize = 18)\n",
    "plt.tight_layout()\n",
    "#plt.savefig('activation_profile_h1.pdf', format = 'pdf', bbox_inches = 'tight') \n",
    "plt.show()\n",
    "\n",
    "hook_handle_2 = net2.fc2.register_forward_hook(get_activation)\n",
    "\n",
    "### Analyze activations\n",
    "net2 = net2.to(device)\n",
    "net2.eval()\n",
    "with torch.no_grad():\n",
    "    x1 = torch.tensor([10]).float().to(device)\n",
    "    y1 = net2(x1)\n",
    "    z1 = activation\n",
    "    x2 = torch.tensor([-100]).float().to(device)\n",
    "    y2 = net2(x2)\n",
    "    z2 = activation\n",
    "    x3 = torch.tensor([200]).float().to(device)\n",
    "    y3 = net2(x3)\n",
    "    z3 = activation\n",
    "\n",
    "### Remove hook\n",
    "hook_handle_2.remove()\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize=(12,6))\n",
    "axs[0].stem(z1.cpu().numpy(), use_line_collection=True)\n",
    "axs[0].set_title('Last layer activations for input x=%.2f' % x1, fontsize = 18)\n",
    "axs[1].stem(z2.cpu().numpy(), use_line_collection=True)\n",
    "axs[1].set_title('Last layer activations for input x=%.2f' % x2, fontsize = 18)\n",
    "axs[2].stem(z3.cpu().numpy(), use_line_collection=True)\n",
    "axs[2].set_title('Last layer activations for input x=%.2f' % x3, fontsize = 18)\n",
    "plt.tight_layout()\n",
    "#plt.savefig('activation_profile_h2.pdf', format = 'pdf', bbox_inches = 'tight') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7Kh7-KqEygq"
   },
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 552
    },
    "id": "PHNS5PRcEygq",
    "outputId": "9fb63bb0-7a27-4149-cd0e-8f667cb2abb1"
   },
   "outputs": [],
   "source": [
    "for sample_batched in test_dataloader:\n",
    "    # Move data to device\n",
    "    x_batch = sample_batched[0].to(device)\n",
    "    label_batch = sample_batched[1].to(device)\n",
    "\n",
    "    net2.eval()\n",
    "    with torch.no_grad(): # turn off gradients computation\n",
    "        y_vec = net2(x_batch)\n",
    "        error = nn.functional.mse_loss(y_vec, label_batch) #Compute the mean squared error\n",
    "        \n",
    "        y_vec = y_vec.cpu().numpy()\n",
    "        label_batch = label_batch.cpu().numpy()\n",
    "        # Plot output\n",
    "        plt.figure(figsize=(12,8))\n",
    "        plt.plot(y_vec, label='Network output')\n",
    "        plt.plot(label_batch, label='True model')\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('y')\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        #plt.savefig('result_regression.pdf', format = 'pdf', bbox_inches = 'tight') \n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "print('#################')\n",
    "print(f'# MSE {error}')\n",
    "print('#################')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HW1_regression_v2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
