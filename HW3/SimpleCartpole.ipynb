{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a490227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from torch import nn\n",
    "from collections import deque # this python module implements exactly what we need for the replay memeory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e6ad58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "trained = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "727fdce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Check if the GPU is available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Training device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45a7120f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity) # Define a queue with maxlen \"capacity\"\n",
    "\n",
    "    def push(self, state, action, next_state, reward):\n",
    "        self.memory.append((state, action, next_state, reward))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch_size = min(batch_size, len(self)) # Get all the samples if the requested batch_size is higher than the number of sample currently in the memory\n",
    "        return random.sample(self.memory, batch_size) # Randomly select \"batch_size\" samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory) # Return the number of samples currently stored in the memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f02329",
   "metadata": {},
   "source": [
    "### Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7aadc45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, state_space_dim, action_space_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(state_space_dim, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, action_space_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de92da93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an example network\n",
    "net = DQN(state_space_dim=4, action_space_dim=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f264b9",
   "metadata": {},
   "source": [
    "### $\\epsilon$-greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84569fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action_epsilon_greedy(net, state, epsilon):\n",
    "    \n",
    "    if epsilon > 1 or epsilon < 0:\n",
    "        raise Exception('The epsilon value must be between 0 and 1')\n",
    "                \n",
    "    # Evaluate the network output from the current state\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        state = torch.tensor(state, dtype=torch.float32) # Convert the state to tensor\n",
    "        net_out = net(state)\n",
    "\n",
    "    # Get the best action (argmax of the network output)\n",
    "    best_action = int(net_out.argmax())\n",
    "    # Get the number of possible actions\n",
    "    action_space_dim = net_out.shape[-1]\n",
    "\n",
    "    # Select a non optimal action with probability epsilon, otherwise choose the best action\n",
    "    if random.random() < epsilon:\n",
    "        # List of non-optimal actions\n",
    "        non_optimal_actions = [a for a in range(action_space_dim) if a != best_action]\n",
    "        # Select randomly\n",
    "        action = random.choice(non_optimal_actions)\n",
    "    else:\n",
    "        # Select best action\n",
    "        action = best_action\n",
    "        \n",
    "    return action, net_out.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd76f01",
   "metadata": {},
   "source": [
    "### Exploration profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c511b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define exploration profile\n",
    "initial_value = 2.5\n",
    "max_value = 1\n",
    "num_iterations = 500\n",
    "exp_decay = np.exp(-np.log(initial_value) / num_iterations * 10) # We compute the exponential decay in such a way the shape of the exploration profile does not depend on the number of iterations\n",
    "exploration_profile = [max_value * (exp_decay ** i) for i in range(num_iterations)]\n",
    "\n",
    "### Plot exploration profile\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(exploration_profile)\n",
    "plt.grid()\n",
    "plt.xlabel('Iteration', fontsize = 14)\n",
    "plt.ylabel('Exploration profile (Epsilon)', fontsize = 14)\n",
    "plt.savefig(\"Exploration_profile_simple_cartpole.pdf\", format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4664db",
   "metadata": {},
   "source": [
    "## CartPole-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5870d741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATE SPACE SIZE: 4\n",
      "ACTION SPACE SIZE: 2\n"
     ]
    }
   ],
   "source": [
    "### Create environment\n",
    "env = gym.make('CartPole-v1') # Initialize the Gym environment\n",
    "env.seed(0) # Set a random seed for the environment (reproducible results)\n",
    "\n",
    "# Get the shapes of the state space (observation_space) and action space (action_space)\n",
    "state_space_dim = env.observation_space.shape[0]\n",
    "action_space_dim = env.action_space.n\n",
    "\n",
    "print(f\"STATE SPACE SIZE: {state_space_dim}\")\n",
    "print(f\"ACTION SPACE SIZE: {action_space_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66962cb",
   "metadata": {},
   "source": [
    "### Network update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02c21df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "### PARAMETERS\n",
    "gamma = 0.999   # gamma parameter for the long term reward\n",
    "replay_memory_capacity = 10000   # Replay memory capacity\n",
    "lr = 5e-4   # Optimizer learning rate\n",
    "target_net_update_steps = 5   # Number of episodes to wait before updating the target network\n",
    "batch_size = 64   # Number of samples to take from the replay memory for each update\n",
    "bad_state_penalty = -100   # Penalty to the reward when we are in a bad state (in this case when the pole falls down) \n",
    "min_samples_for_training = 1000   # Minimum samples in the replay memory to enable the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a953e487",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialize the replay memory\n",
    "replay_mem = ReplayMemory(replay_memory_capacity)    \n",
    "\n",
    "### Initialize the policy network\n",
    "policy_net = DQN(state_space_dim, action_space_dim)\n",
    "\n",
    "### Initialize the target network with the same weights of the policy network\n",
    "target_net = DQN(state_space_dim, action_space_dim)\n",
    "target_net.load_state_dict(policy_net.state_dict()) # This will copy the weights of the policy network to the target network\n",
    "\n",
    "### Initialize the optimizer\n",
    "optimizer = torch.optim.Adam(policy_net.parameters(), lr=lr) # The optimizer will update ONLY the parameters of the policy network\n",
    "\n",
    "### Initialize the loss function\n",
    "loss_fn = nn.SmoothL1Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5a17ba",
   "metadata": {},
   "source": [
    "### Update function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d376e821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_step(policy_net, target_net, replay_mem, gamma, optimizer, loss_fn, batch_size):\n",
    "        \n",
    "    # Sample the data from the replay memory\n",
    "    batch = replay_mem.sample(batch_size)\n",
    "    batch_size = len(batch)\n",
    "    # Create tensors for each element of the batch\n",
    "    states      = torch.tensor(np.array([s[0] for s in batch]), dtype=torch.float32)\n",
    "    actions     = torch.tensor(np.array([s[1] for s in batch]), dtype=torch.int64)\n",
    "    rewards     = torch.tensor(np.array([s[3] for s in batch]), dtype=torch.float32)\n",
    "\n",
    "    # Compute a mask of non-final states (all the elements where the next state is not None)\n",
    "    non_final_next_states = torch.tensor(np.array([s[2] for s in batch if s[2] is not None]), dtype=torch.float32) # the next state can be None if the game has ended\n",
    "    non_final_mask = torch.tensor([s[2] is not None for s in batch], dtype=torch.bool)\n",
    "\n",
    "    # Compute all the Q values (forward pass)\n",
    "    policy_net.train()\n",
    "    q_values = policy_net(states)\n",
    "    # Select the proper Q value for the corresponding action taken Q(s_t, a)\n",
    "    state_action_values = q_values.gather(1, actions.unsqueeze(1))\n",
    "\n",
    "    # Compute the value function of the next states using the target network V(s_{t+1}) = max_a( Q_target(s_{t+1}, a)) )\n",
    "    with torch.no_grad():\n",
    "        target_net.eval()\n",
    "        q_values_target = target_net(non_final_next_states)\n",
    "    # For the terminal state the expected reward is zero otherwise is set to the max computed with the target net\n",
    "    next_state_max_q_values = torch.zeros(batch_size)\n",
    "    next_state_max_q_values[non_final_mask] = q_values_target.max(dim=1)[0]\n",
    "\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = rewards + (next_state_max_q_values * gamma)\n",
    "    expected_state_action_values = expected_state_action_values.unsqueeze(1) # Set the required tensor shape\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = loss_fn(state_action_values, expected_state_action_values)\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # Apply gradient clipping (clip all the gradients greater than 2 for training stability)\n",
    "    nn.utils.clip_grad_norm_(policy_net.parameters(), 2)\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab1291c",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088960a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not trained:  \n",
    "    # Initialize the Gym environment\n",
    "    env = gym.make('CartPole-v1') \n",
    "    env.seed(0) # Set a random seed for the environment (reproducible results)\n",
    "\n",
    "    returns, mean_ret = [], []\n",
    "    patience = 100\n",
    "    for episode_num, epsilon in enumerate(tqdm(exploration_profile)):\n",
    "\n",
    "        # Reset the environment and get the initial state\n",
    "        state = env.reset()\n",
    "        # Reset the score. The final score will be the total amount of steps before the pole falls\n",
    "        score = 0\n",
    "        done = False\n",
    "\n",
    "        episode_loss = []\n",
    "\n",
    "        # Go on until the pole falls off\n",
    "        while not done:\n",
    "\n",
    "            # Choose the action following the policy\n",
    "            action, q_values = choose_action_epsilon_greedy(policy_net, state, epsilon)\n",
    "\n",
    "            # Apply the action and get the next state, the reward and a flag \"done\" that is True if the game is ended\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "            # We apply a (linear) penalty when the cart is far from center\n",
    "            pos_weight = 1\n",
    "            reward = reward - pos_weight * np.abs(state[0]) \n",
    "\n",
    "            # We apply also a penalty on the pole angle to stabilize better the system\n",
    "            angle_weight = 0.5\n",
    "            reward = reward - angle_weight*np.abs(state[3])\n",
    "\n",
    "            # Update the final score (+1 for each step)\n",
    "            score += 1\n",
    "\n",
    "            # Apply penalty for bad state\n",
    "            if done: # if the pole has fallen down \n",
    "                reward += bad_state_penalty\n",
    "                next_state = None\n",
    "                returns.append(score)\n",
    "                mean_ret.append(sum(returns[-100:]) / len(returns[-100:]))\n",
    "                if mean_ret[-1] >= 475:\n",
    "                    patience -= 1\n",
    "                else: patience = 100\n",
    "\n",
    "            # Update the replay memory\n",
    "            replay_mem.push(state, action, next_state, reward)\n",
    "            # Update the network\n",
    "            if len(replay_mem) > min_samples_for_training: # we enable the training only if we have enough samples in the replay memory, otherwise the training will use the same samples too often\n",
    "                loss = update_step(policy_net, target_net, replay_mem, gamma, optimizer, loss_fn, batch_size)\n",
    "                episode_loss.append(loss.detach().numpy())\n",
    "            # Visually render the environment (disable to speed up the training)\n",
    "            #env.render()\n",
    "\n",
    "            # Set the current state for the next iteration\n",
    "            state = next_state\n",
    "\n",
    "        if patience == 0: \n",
    "            print(\"The task has been solved after \" + str(episode_num) + \" episodes\")\n",
    "            break\n",
    "\n",
    "        # Update the target network every target_net_update_steps episodes\n",
    "        if episode_num % target_net_update_steps == 0:\n",
    "            print('Updating target network...')\n",
    "            target_net.load_state_dict(policy_net.state_dict()) # This will copy the weights of the policy network to the target network\n",
    "            torch.save(target_net.state_dict(), 'Cartpole-DQL.torch')\n",
    "\n",
    "        # Print the final score\n",
    "        print(f\"EPISODE: {episode_num + 1} - FINAL SCORE: {score} - Epsilon: {epsilon}\") # Print the final score\n",
    "\n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.plot(mean_ret, 'r')\n",
    "    plt.plot(returns, 'b', alpha = 0.3)\n",
    "    plt.ylabel('Score', fontsize=18)\n",
    "    plt.xlabel('Episode', fontsize=18)\n",
    "    plt.savefig(\"Score_cartpole.pdf\", format='pdf')\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ded76ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-15 15:46:01.121 python[14794:1220052] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7f9d014ea390>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-03-15 15:46:01.122 python[14794:1220052] Warning: Expected min height of view: (<NSButton: 0x7f9d014fc6c0>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-03-15 15:46:01.124 python[14794:1220052] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7f9d02854580>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-03-15 15:46:01.126 python[14794:1220052] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7f9d033fd060>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE 1 - FINAL SCORE: 500.0\n",
      "EPISODE 2 - FINAL SCORE: 500.0\n",
      "EPISODE 3 - FINAL SCORE: 500.0\n",
      "EPISODE 4 - FINAL SCORE: 500.0\n",
      "EPISODE 5 - FINAL SCORE: 500.0\n",
      "EPISODE 6 - FINAL SCORE: 500.0\n",
      "EPISODE 7 - FINAL SCORE: 500.0\n",
      "EPISODE 8 - FINAL SCORE: 500.0\n",
      "EPISODE 9 - FINAL SCORE: 500.0\n",
      "EPISODE 10 - FINAL SCORE: 500.0\n",
      "EPISODE 11 - FINAL SCORE: 500.0\n",
      "EPISODE 12 - FINAL SCORE: 500.0\n",
      "EPISODE 13 - FINAL SCORE: 500.0\n",
      "EPISODE 14 - FINAL SCORE: 500.0\n",
      "EPISODE 15 - FINAL SCORE: 500.0\n",
      "EPISODE 16 - FINAL SCORE: 500.0\n",
      "EPISODE 17 - FINAL SCORE: 500.0\n",
      "EPISODE 18 - FINAL SCORE: 500.0\n",
      "EPISODE 19 - FINAL SCORE: 500.0\n",
      "EPISODE 20 - FINAL SCORE: 500.0\n",
      "EPISODE 21 - FINAL SCORE: 500.0\n",
      "EPISODE 22 - FINAL SCORE: 500.0\n",
      "EPISODE 23 - FINAL SCORE: 500.0\n",
      "EPISODE 24 - FINAL SCORE: 500.0\n",
      "EPISODE 25 - FINAL SCORE: 500.0\n",
      "EPISODE 26 - FINAL SCORE: 500.0\n",
      "EPISODE 27 - FINAL SCORE: 500.0\n",
      "EPISODE 28 - FINAL SCORE: 500.0\n",
      "EPISODE 29 - FINAL SCORE: 500.0\n",
      "EPISODE 30 - FINAL SCORE: 500.0\n",
      "EPISODE 31 - FINAL SCORE: 500.0\n",
      "EPISODE 32 - FINAL SCORE: 500.0\n",
      "EPISODE 33 - FINAL SCORE: 500.0\n",
      "EPISODE 34 - FINAL SCORE: 500.0\n",
      "EPISODE 35 - FINAL SCORE: 500.0\n",
      "EPISODE 36 - FINAL SCORE: 500.0\n",
      "EPISODE 37 - FINAL SCORE: 500.0\n",
      "EPISODE 38 - FINAL SCORE: 500.0\n",
      "EPISODE 39 - FINAL SCORE: 500.0\n",
      "EPISODE 40 - FINAL SCORE: 500.0\n",
      "EPISODE 41 - FINAL SCORE: 500.0\n",
      "EPISODE 42 - FINAL SCORE: 500.0\n",
      "EPISODE 43 - FINAL SCORE: 500.0\n",
      "EPISODE 44 - FINAL SCORE: 500.0\n",
      "EPISODE 45 - FINAL SCORE: 500.0\n",
      "EPISODE 46 - FINAL SCORE: 500.0\n",
      "EPISODE 47 - FINAL SCORE: 500.0\n",
      "EPISODE 48 - FINAL SCORE: 500.0\n",
      "EPISODE 49 - FINAL SCORE: 500.0\n",
      "EPISODE 50 - FINAL SCORE: 500.0\n",
      "EPISODE 51 - FINAL SCORE: 500.0\n",
      "EPISODE 52 - FINAL SCORE: 500.0\n",
      "EPISODE 53 - FINAL SCORE: 500.0\n",
      "EPISODE 54 - FINAL SCORE: 500.0\n",
      "EPISODE 55 - FINAL SCORE: 500.0\n",
      "EPISODE 56 - FINAL SCORE: 500.0\n",
      "EPISODE 57 - FINAL SCORE: 500.0\n",
      "EPISODE 58 - FINAL SCORE: 500.0\n",
      "EPISODE 59 - FINAL SCORE: 500.0\n",
      "EPISODE 60 - FINAL SCORE: 500.0\n",
      "EPISODE 61 - FINAL SCORE: 500.0\n",
      "EPISODE 62 - FINAL SCORE: 500.0\n",
      "EPISODE 63 - FINAL SCORE: 500.0\n",
      "EPISODE 64 - FINAL SCORE: 500.0\n",
      "EPISODE 65 - FINAL SCORE: 500.0\n",
      "EPISODE 66 - FINAL SCORE: 500.0\n",
      "EPISODE 67 - FINAL SCORE: 500.0\n",
      "EPISODE 68 - FINAL SCORE: 500.0\n",
      "EPISODE 69 - FINAL SCORE: 500.0\n",
      "EPISODE 70 - FINAL SCORE: 500.0\n",
      "EPISODE 71 - FINAL SCORE: 500.0\n",
      "EPISODE 72 - FINAL SCORE: 500.0\n",
      "EPISODE 73 - FINAL SCORE: 500.0\n",
      "EPISODE 74 - FINAL SCORE: 500.0\n",
      "EPISODE 75 - FINAL SCORE: 500.0\n",
      "EPISODE 76 - FINAL SCORE: 500.0\n",
      "EPISODE 77 - FINAL SCORE: 500.0\n",
      "EPISODE 78 - FINAL SCORE: 500.0\n",
      "EPISODE 79 - FINAL SCORE: 500.0\n",
      "EPISODE 80 - FINAL SCORE: 500.0\n",
      "EPISODE 81 - FINAL SCORE: 500.0\n",
      "EPISODE 82 - FINAL SCORE: 500.0\n",
      "EPISODE 83 - FINAL SCORE: 500.0\n",
      "EPISODE 84 - FINAL SCORE: 500.0\n",
      "EPISODE 85 - FINAL SCORE: 500.0\n",
      "EPISODE 86 - FINAL SCORE: 500.0\n",
      "EPISODE 87 - FINAL SCORE: 500.0\n",
      "EPISODE 88 - FINAL SCORE: 500.0\n",
      "EPISODE 89 - FINAL SCORE: 500.0\n",
      "EPISODE 90 - FINAL SCORE: 500.0\n",
      "EPISODE 91 - FINAL SCORE: 500.0\n",
      "EPISODE 92 - FINAL SCORE: 500.0\n",
      "EPISODE 93 - FINAL SCORE: 500.0\n",
      "EPISODE 94 - FINAL SCORE: 500.0\n",
      "EPISODE 95 - FINAL SCORE: 500.0\n",
      "EPISODE 96 - FINAL SCORE: 500.0\n",
      "EPISODE 97 - FINAL SCORE: 500.0\n",
      "EPISODE 98 - FINAL SCORE: 500.0\n",
      "EPISODE 99 - FINAL SCORE: 500.0\n",
      "EPISODE 100 - FINAL SCORE: 500.0\n",
      "MEAN SCORE OVER 100 EPISODES IS: 500.0\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Gym environment\n",
    "env = gym.make('CartPole-v1') \n",
    "env.seed(0) # Set a random seed for the environment (reproducible results)\n",
    "returns = []\n",
    "policy_net.load_state_dict(torch.load('Cartpole-DQL.torch'))\n",
    "for num_episode in range(100): \n",
    "    # Reset the environment and get the initial state\n",
    "    state = env.reset()\n",
    "    # Reset the score. The final score will be the total amount of steps before the pole falls\n",
    "    score = 0\n",
    "    done = False\n",
    "    # Go on until the pole falls off or the score reach 490\n",
    "    while not done:\n",
    "      # Choose the best action (epsilon 0)\n",
    "      action, q_values = choose_action_epsilon_greedy(policy_net, state, epsilon=0)\n",
    "      # Apply the action and get the next state, the reward and a flag \"done\" that is True if the game is ended\n",
    "      next_state, reward, done, info = env.step(action)\n",
    "      # Visually render the environment\n",
    "      env.render()\n",
    "      # Update the final score (+1 for each step)\n",
    "      score += reward \n",
    "      # Set the current state for the next iteration\n",
    "      state = next_state\n",
    "      # Check if the episode ended (the pole fell down)\n",
    "    returns.append(score)\n",
    "    print(f\"EPISODE {num_episode + 1} - FINAL SCORE: {score}\") \n",
    "print(f\"MEAN SCORE OVER 100 EPISODES IS: {np.mean(returns)}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0cd762",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
