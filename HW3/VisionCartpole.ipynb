{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mjVIDzI12BGI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mjVIDzI12BGI",
    "outputId": "3f07aaef-3b36-4db5-fb65-84ec218f922f"
   },
   "outputs": [],
   "source": [
    "### This cell is needed to run the code in Colab\n",
    "!apt-get install -y xvfb x11-utils\n",
    "!pip install pyvirtualdisplay==0.2.* \\\n",
    "             PyOpenGL==3.1.* \\\n",
    "             PyOpenGL-accelerate==3.1.*\n",
    "import pyvirtualdisplay\n",
    "\n",
    "\n",
    "_display = pyvirtualdisplay.Display(visible=False,  # use False with Xvfb\n",
    "                                    size=(1400, 900))\n",
    "_ = _display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84abda6",
   "metadata": {
    "id": "f84abda6"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from collections import deque # this python module implements exactly what we need for the replay memeory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7KHlM6HqkIf0",
   "metadata": {
    "id": "7KHlM6HqkIf0"
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb099822",
   "metadata": {
    "id": "bb099822"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "trained = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45fed4c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d45fed4c",
    "outputId": "04b5e8f0-277d-4faf-c0db-669140a419cc"
   },
   "outputs": [],
   "source": [
    "# Check if the GPU is available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Training device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2fe13e",
   "metadata": {
    "id": "1a2fe13e"
   },
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity) # Define a queue with maxlen \"capacity\"\n",
    "\n",
    "    def push(self, state, action, next_state, reward):\n",
    "        self.memory.append((state, action, next_state, reward))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch_size = min(batch_size, len(self)) # Get all the samples if the requested batch_size is higher than the number of sample currently in the memory\n",
    "        return random.sample(self.memory, batch_size) # Randomly select \"batch_size\" samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory) # Return the number of samples currently stored in the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00af12a",
   "metadata": {
    "id": "c00af12a"
   },
   "outputs": [],
   "source": [
    "class ConvDQN(nn.Module):\n",
    "\n",
    "    def __init__(self, outputs, w=40, h=60):\n",
    "        super(ConvDQN, self).__init__()\n",
    "        self.conv = nn.Sequential(nn.Conv2d(4, 16, kernel_size=5, stride=3), nn.BatchNorm2d(16), nn.ReLU(True),\n",
    "                                  nn.Conv2d(16, 32, kernel_size=4, stride=2), nn.BatchNorm2d(32), nn.ReLU(True))\n",
    "                                  #nn.Conv2d(32, 64, kernel_size=3, stride=1), nn.BatchNorm2d(64), nn.ReLU(True))\n",
    "\n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out1(size, kernel_size = 5, stride = 3):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        def conv2d_size_out2(size, kernel_size = 4, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        # def conv2d_size_out3(size, kernel_size = 3, stride = 1):\n",
    "        #     return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convh = conv2d_size_out2(conv2d_size_out1(h))\n",
    "        convw = conv2d_size_out2(conv2d_size_out1(w))\n",
    "        linear_input_size = convh * convw * 32\n",
    "        self.head = nn.Sequential(nn.Linear(linear_input_size, 512), nn.ReLU(True), \n",
    "                                  nn.Linear(512, 256), nn.ReLU(True), \n",
    "                                  nn.Linear(256, outputs))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        flatten = x.view(x.size(0), -1)\n",
    "        return self.head(flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9a1130",
   "metadata": {
    "id": "6a9a1130"
   },
   "outputs": [],
   "source": [
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize((40,60), interpolation=T.InterpolationMode.BICUBIC), \n",
    "                    T.ToTensor(),\n",
    "                    T.Grayscale(num_output_channels=1)])\n",
    "\n",
    "class Agent:\n",
    "    \n",
    "    def __init__(self, env_name):\n",
    "        self.env_name = env_name\n",
    "        self.env = gym.make(env_name)\n",
    "        self.env.seed(0)\n",
    "        self.ROWS = 40\n",
    "        self.COLS = 60\n",
    "        self.REM_STEP = 4\n",
    "        self.image_memory = torch.zeros(self.REM_STEP, self.ROWS, self.COLS)\n",
    "        self.n_actions = self.env.action_space.n\n",
    "\n",
    "    def get_cart_location(self, screen_width):\n",
    "        world_width = self.env.x_threshold * 2\n",
    "        scale = screen_width / world_width\n",
    "        return int(self.env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "    \n",
    "    def get_screen(self):\n",
    "        # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
    "        # such as 800x1200x3. Transpose it into torch order (CHW).\n",
    "        screen = self.env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "        # Cart is in the lower half, so strip off the top and bottom of the screen\n",
    "        _, screen_height, screen_width = screen.shape\n",
    "        screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]#crop top and bottom\n",
    "        view_width = int(screen_width * 0.6)\n",
    "        cart_location = self.get_cart_location(screen_width)\n",
    "        if cart_location < view_width // 2:\n",
    "            slice_range = slice(view_width)\n",
    "        elif cart_location > (screen_width - view_width // 2):\n",
    "            slice_range = slice(-view_width, None)\n",
    "        else:\n",
    "            slice_range = slice(cart_location - view_width // 2,\n",
    "                                cart_location + view_width // 2)   \n",
    "        # Strip off the edges\n",
    "        screen = screen[:, :, slice_range]\n",
    "        # Convert to float, rescale, convert to torch tensor\n",
    "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "        screen = torch.from_numpy(screen) # to tensor\n",
    "        # Roll in the image memory to discard the oldest image\n",
    "        self.image_memory = torch.roll(self.image_memory, 1, dims = 0)\n",
    "        screen = resize(screen) # Apply the composed transform\n",
    "        # insert in the first position the latest screen\n",
    "        self.image_memory[0,:,:] = screen  \n",
    "        #return the memory after having added the batch dimension\n",
    "        return self.image_memory.unsqueeze(0)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        for i in range(self.REM_STEP):\n",
    "            state= self.get_screen()\n",
    "        return state\n",
    "        \n",
    "    def step(self,action):\n",
    "        _, reward, done, _ = self.env.step(action)\n",
    "        next_state = self.get_screen()\n",
    "        return next_state, reward, done\n",
    "    \n",
    "    def render(self):\n",
    "        self.env.render()\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d15791",
   "metadata": {
    "id": "90d15791"
   },
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v0'\n",
    "agent = Agent(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d420dcc6",
   "metadata": {
    "id": "d420dcc6"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "gamma = 0.999\n",
    "learning_rate = 5e-5\n",
    "bad_state_penalty = -100\n",
    "target_net_update_steps = 25\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = agent.n_actions\n",
    "\n",
    "policy_net = ConvDQN(n_actions).to(device)\n",
    "target_net = ConvDQN(n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = torch.optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "replay_mem = ReplayMemory(100000)\n",
    "min_samples_for_training = 5000\n",
    "\n",
    "loss_fn = nn.SmoothL1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f046e6b6",
   "metadata": {
    "id": "f046e6b6"
   },
   "outputs": [],
   "source": [
    "def choose_action_epsilon_greedy(net, state, epsilon):\n",
    "    \n",
    "    if epsilon > 1 or epsilon < 0:\n",
    "        raise Exception('The epsilon value must be between 0 and 1')\n",
    "                \n",
    "    # Evaluate the network output from the current state\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        net_out = net(state.to(device))\n",
    "        \n",
    "    # Get the best action (argmax of the network output)\n",
    "    best_action = int(net_out.argmax())\n",
    "    # Get the number of possible actions\n",
    "    action_space_dim = net_out.shape[-1]\n",
    "\n",
    "    # Select a non optimal action with probability epsilon, otherwise choose the best action\n",
    "    if random.random() < epsilon:\n",
    "        # List of non-optimal actions\n",
    "        non_optimal_actions = [a for a in range(action_space_dim) if a != best_action]\n",
    "        # Select randomly\n",
    "        action = random.choice(non_optimal_actions)\n",
    "    else:\n",
    "        # Select best action\n",
    "        action = best_action\n",
    "        \n",
    "    return action, net_out.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14677d0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 515
    },
    "id": "b14677d0",
    "outputId": "f18b14b4-7451-4cea-9e74-03ced0c2bad7"
   },
   "outputs": [],
   "source": [
    "### Define exploration profile\n",
    "max_value = 1\n",
    "num_iterations = 2000\n",
    "exp_decay = 0.01\n",
    "exploration_profile = [max_value * np.exp(-exp_decay * i) for i in range(num_iterations)]\n",
    "exploration_profile = [0.01 if x < 0.01 else x for x in exploration_profile] # set minimum exploring rate\n",
    "\n",
    "\n",
    "### Plot exploration profile\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(exploration_profile)\n",
    "plt.grid()\n",
    "plt.xlabel('Iteration', fontsize = 14)\n",
    "plt.ylabel('Exploration profile (Epsilon)', fontsize = 14)\n",
    "#plt.savefig(\"Exploration_vision.pdf\", format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5727e889",
   "metadata": {
    "id": "5727e889"
   },
   "outputs": [],
   "source": [
    "def update_step(policy_net, target_net, buffer, gamma, batch_size, loss_fn, optimizer):\n",
    "    # Sample a batch of size batch_size from the buffer\n",
    "    batch = buffer.sample(batch_size)\n",
    "    \n",
    "    states = torch.cat([s[0] for s in batch])\n",
    "    actions = torch.tensor([s[1] for s in batch], dtype=torch.int64, device=device)\n",
    "    rewards = torch.tensor([s[3] for s in batch], dtype=torch.float32, device=device)\n",
    "    \n",
    "    # Compute a mask of non-final states (all the elements where the next state is not None)\n",
    "    non_final_mask = torch.tensor([s[2] is not None for s in batch], dtype=torch.bool, device=device)\n",
    "    non_final_next_states = torch.cat([s[2] for s in batch if s[2] is not None])\n",
    "    \n",
    "    # Compute the action values from the batch states\n",
    "    policy_net.train()\n",
    "    q_values = policy_net(states.to(device))\n",
    "    # Select the proper Q value for the corresponding action taken Q(s_t, a)\n",
    "    state_action_values = q_values.gather(1, actions.unsqueeze(1))\n",
    "    \n",
    "    # Compute the value function of the next states using the target network V(s_{t+1}) = max_a( Q_target(s_{t+1}, a)) )\n",
    "    with torch.no_grad():\n",
    "        target_net.eval()\n",
    "        q_values_target = target_net(non_final_next_states.to(device))\n",
    "    # For the terminal state the expected reward is zero otherwise is set to the max computed with the target net\n",
    "    next_state_max_q_values = torch.zeros(batch_size, device=device)\n",
    "    next_state_max_q_values[non_final_mask] = q_values_target.max(dim=1)[0]\n",
    "    \n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = rewards + (next_state_max_q_values * gamma)\n",
    "    expected_state_action_values = expected_state_action_values.unsqueeze(1) # Set the required tensor shape\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = loss_fn(state_action_values, expected_state_action_values)\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # Apply gradient clipping (clip all the gradients greater than 2 for training stability)\n",
    "    nn.utils.clip_grad_norm_(policy_net.parameters(), 2)\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b12677",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14142d1a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "2aa3689c9d7b4b20895508e898e518ca",
      "731aa7ac33dd4052bd434bd2a7db82a1",
      "064bea0143724756b5aae8dbb7086611",
      "0ac0084e94924150bd9dd2d2b7389dbb",
      "49b081f20fdf4ca7901329dbffa0f8b0",
      "15fec1a0216e417085fcc9d09fb29c26",
      "7a04549706404bb882c46d0c4466139d",
      "60f7241949c847eea9db85c2b8ac6910",
      "66a3ef3fac3b4899abd3fabf1c1ebd4b",
      "a64e90794f9449e9ba48da113ed5cf1a",
      "226929734ae04c759bd7a3887deafb9d"
     ]
    },
    "id": "14142d1a",
    "outputId": "a43feda8-9187-4994-c7e6-662fee8f3a04"
   },
   "outputs": [],
   "source": [
    "if not trained:\n",
    "    rewards = []\n",
    "    mean_rew = []\n",
    "    patience = 10\n",
    "    for episode_num, epsilon in enumerate(tqdm(exploration_profile)):\n",
    "\n",
    "        state = agent.reset()\n",
    "        # Reset the score. The final score will be the total amount of steps before the pole falls\n",
    "        score = 0\n",
    "        done = False\n",
    "\n",
    "        # Go on until the pole falls off\n",
    "        while not done:\n",
    "\n",
    "            # Choose the action following the policy\n",
    "            action, _ = choose_action_epsilon_greedy(policy_net, state, epsilon)\n",
    "\n",
    "            # Apply the action to perform a step\n",
    "            next_state, reward, done = agent.step(action)\n",
    "\n",
    "            # Update the final score (+1 for each step)\n",
    "            score += 1\n",
    "\n",
    "            # Apply penalty for bad state\n",
    "            if done: # if the pole has fallen down \n",
    "                reward += bad_state_penalty\n",
    "                next_state = None\n",
    "                rewards.append(score)\n",
    "                mean_rew.append(sum(rewards[-50:]) / len(rewards[-50:]))\n",
    "                if mean_rew[-1] >= 190:\n",
    "                    patience -= 1\n",
    "                else: patience = 10\n",
    "            if patience == 0:\n",
    "                print(\"The game has been solved\")\n",
    "                break\n",
    "\n",
    "            # Update the replay memory\n",
    "            replay_mem.push(state, action, next_state, reward)\n",
    "\n",
    "            # Update the network\n",
    "            if len(replay_mem) > min_samples_for_training: # we enable the training only if we have enough samples in the replay memory, otherwise the training will use the same samples too often\n",
    "                update_step(policy_net, target_net, replay_mem, gamma, batch_size, loss_fn, optimizer)\n",
    "            # Visually render the environment (disable to speed up the training)\n",
    "            #env.render()\n",
    "\n",
    "            # Set the current state for the next iteration\n",
    "            state = next_state\n",
    "\n",
    "        # Update the target network every target_net_update_steps episodes\n",
    "        if episode_num % target_net_update_steps == 0:\n",
    "            print('Updating target network...')\n",
    "            target_net.load_state_dict(policy_net.state_dict()) # This will copy the weights of the policy network to the target network\n",
    "            torch.save(target_net.state_dict(), 'RL-DQN.torch')\n",
    "\n",
    "        # Print the final score\n",
    "        print(f\"EPISODE: {episode_num + 1} - FINAL SCORE: {score} - Epsilon: {epsilon}\") # Print the final score\n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.plot(mean_rew, 'r')\n",
    "    plt.plot(rewards, 'b', alpha = 0.3)\n",
    "    plt.ylabel('Score', fontsize=18)\n",
    "    plt.xlabel('Episode', fontsize=18)\n",
    "    plt.savefig(\"score.pdf\", format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ce3345",
   "metadata": {
    "id": "5a3431ab"
   },
   "source": [
    "### Test the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043b14cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Gym environment\n",
    "env_name = 'CartPole-v0'\n",
    "agent = Agent(env_name)\n",
    "n_actions = agent.n_actions\n",
    "policy_net = ConvDQN(n_actions).to(device)\n",
    "policy_net.load_state_dict(torch.load('RL-DQN.torch', map_location=device))\n",
    "returns = []\n",
    "# Let's try for a total of 10 episodes\n",
    "for num_episode in range(100): \n",
    "    # Reset the environment and get the initial state\n",
    "    state = agent.reset()\n",
    "    # Reset the score. The final score will be the total amount of steps before the pole falls\n",
    "    score = 0\n",
    "    done = False\n",
    "    # Go on until the pole falls off or the score reach 490\n",
    "    while not done:\n",
    "      # Choose the best action (epsilon 0)\n",
    "      action, _ = choose_action_epsilon_greedy(policy_net, state, epsilon=0)\n",
    "      # Apply the action and get the next state, the reward and a flag \"done\" that is True if the game is ended\n",
    "      next_state, reward, done = agent.step(action)\n",
    "      # Visually render the environment\n",
    "      agent.render()\n",
    "      # Update the final score (+1 for each step)\n",
    "      score += reward \n",
    "      # Set the current state for the next iteration\n",
    "      state = next_state\n",
    "      # Check if the episode ended (the pole fell down)\n",
    "    # Print the final score\n",
    "    returns.append(score)\n",
    "    print(f\"EPISODE {num_episode + 1} - FINAL SCORE: {score}\") \n",
    "print(f\"MEAN SCORE OVER 100 EPISODES IS: {np.mean(returns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93292852",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cartpole.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "064bea0143724756b5aae8dbb7086611": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7a04549706404bb882c46d0c4466139d",
      "placeholder": "​",
      "style": "IPY_MODEL_15fec1a0216e417085fcc9d09fb29c26",
      "value": "100%"
     }
    },
    "0ac0084e94924150bd9dd2d2b7389dbb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_66a3ef3fac3b4899abd3fabf1c1ebd4b",
      "max": 2000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_60f7241949c847eea9db85c2b8ac6910",
      "value": 2000
     }
    },
    "15fec1a0216e417085fcc9d09fb29c26": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "226929734ae04c759bd7a3887deafb9d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2aa3689c9d7b4b20895508e898e518ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_064bea0143724756b5aae8dbb7086611",
       "IPY_MODEL_0ac0084e94924150bd9dd2d2b7389dbb",
       "IPY_MODEL_49b081f20fdf4ca7901329dbffa0f8b0"
      ],
      "layout": "IPY_MODEL_731aa7ac33dd4052bd434bd2a7db82a1"
     }
    },
    "49b081f20fdf4ca7901329dbffa0f8b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_226929734ae04c759bd7a3887deafb9d",
      "placeholder": "​",
      "style": "IPY_MODEL_a64e90794f9449e9ba48da113ed5cf1a",
      "value": " 2000/2000 [49:08&lt;00:00,  3.28s/it]"
     }
    },
    "60f7241949c847eea9db85c2b8ac6910": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "66a3ef3fac3b4899abd3fabf1c1ebd4b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "731aa7ac33dd4052bd434bd2a7db82a1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7a04549706404bb882c46d0c4466139d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a64e90794f9449e9ba48da113ed5cf1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
