{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dbfbe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torch import nn\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e971fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d85e4ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") \n",
    "trained = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50eb3b04",
   "metadata": {},
   "source": [
    "### Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c79c845",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity) # Define a queue with maxlen \"capacity\"\n",
    "\n",
    "    def push(self, state, action, next_state, reward):\n",
    "        self.memory.append((state, action, next_state, reward))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch_size = min(batch_size, len(self)) # Get all the samples if the requested batch_size is higher than the number of sample currently in the memory\n",
    "        return random.sample(self.memory, batch_size) # Randomly select \"batch_size\" samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory) # Return the number of samples currently stored in the memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7d1487",
   "metadata": {},
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e9f0681",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_space_dim, action_space_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Sequential(nn.Linear(state_space_dim, 512), nn.ReLU(True),\n",
    "                                    nn.Linear(512, 128), nn.ReLU(True),\n",
    "                                    nn.Linear(128, action_space_dim))\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f60150",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0616d9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = .999\n",
    "LR = 1e-3\n",
    "BATCH_SIZE = 64\n",
    "OPTIM_NAME = 'Adam'\n",
    "loss_function = nn.SmoothL1Loss()\n",
    "min_samples_for_training = 1000\n",
    "update_target_every_steps = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91475dd",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6564a1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    def __init__(self, env_name):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.env.seed(0)\n",
    "        self.n_actions = self.env.action_space.n\n",
    "        self.state_space_dim = self.env.observation_space.shape[0]\n",
    "        self.buffer = ReplayMemory(capacity=10000)\n",
    "        self.target_net = DQN(self.state_space_dim, self.n_actions)\n",
    "        self.policy_net = DQN(self.state_space_dim, self.n_actions)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.gamma = GAMMA\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.lr = LR\n",
    "        self.optim_name = OPTIM_NAME\n",
    "        self.optimizer = getattr(torch.optim, self.optim_name)(self.policy_net.parameters(), lr = self.lr)\n",
    "        \n",
    "    def reset(self):\n",
    "        state = self.env.reset()\n",
    "        return state\n",
    "        \n",
    "    def step(self, action):\n",
    "        next_state, reward, done, _ = self.env.step(action)\n",
    "        return next_state, reward, done\n",
    "    \n",
    "    def store(self, state, action, reward, next_state):\n",
    "        self.buffer.push(state, action, reward, next_state)\n",
    "        \n",
    "    def choose_action(self, state, epsilon):\n",
    "        \n",
    "        if epsilon > 1 or epsilon < 0:\n",
    "            raise Exception('The epsilon value must be between 0 and 1')\n",
    "                \n",
    "        # Evaluate the network output from the current state\n",
    "        with torch.no_grad():\n",
    "            self.policy_net.eval()\n",
    "            state = torch.tensor(state, dtype=torch.float32) # Convert the state to tensor\n",
    "            net_out = self.policy_net(state)\n",
    "\n",
    "        # Get the best action (argmax of the network output)\n",
    "        best_action = int(net_out.argmax())\n",
    "\n",
    "        # Select a non optimal action with probability epsilon, otherwise choose the best action\n",
    "        if random.random() < epsilon:\n",
    "            # List of non-optimal actions\n",
    "            non_optimal_actions = [a for a in range(self.n_actions) if a != best_action]\n",
    "            # Select randomly\n",
    "            action = random.choice(non_optimal_actions)\n",
    "        else:\n",
    "            # Select best action\n",
    "            action = best_action\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def learn(self, loss_fn):\n",
    "        # Sample the data from the replay memory\n",
    "        batch = self.buffer.sample(self.batch_size)\n",
    "        \n",
    "        # Create tensors for each element of the batch\n",
    "        states = torch.tensor(np.array([s[0] for s in batch]), dtype=torch.float32)\n",
    "        actions = torch.tensor(np.array([s[1] for s in batch]), dtype=torch.int64)\n",
    "        rewards = torch.tensor(np.array([s[2] for s in batch]), dtype=torch.float32)\n",
    "        \n",
    "        # Compute non-final state mask\n",
    "        non_final_states_mask = torch.tensor([s[3] is not None for s in batch], dtype=torch.bool)\n",
    "        non_final_next_states = torch.tensor(np.array([s[3] for s in batch if s[3] is not None]), dtype=torch.float32)\n",
    "        \n",
    "        #Compute the Q values\n",
    "        self.policy_net.train()\n",
    "        q_values = self.policy_net(states)\n",
    "        \n",
    "        # Select the proper Q value for the corresponding action taken Q(s_t, a)\n",
    "        state_action_values = q_values.gather(1, actions.unsqueeze(1))\n",
    "        \n",
    "        #Compute the Q value for the next state using the target net\n",
    "        with torch.no_grad():\n",
    "            self.target_net.eval()\n",
    "            q_next = self.target_net(non_final_next_states)\n",
    "        #Take the max value of these Q values (choose the greedy action)\n",
    "        q_next_max = torch.zeros(self.batch_size)\n",
    "        q_next_max[non_final_states_mask] = q_next.max(dim=1)[0]\n",
    "        \n",
    "        #Compute TD target\n",
    "        target = rewards+self.gamma*q_next_max\n",
    "        target = target.unsqueeze(1)\n",
    "        \n",
    "        #Compute TD error\n",
    "        error = loss_fn(state_action_values, target)\n",
    "        \n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        error.backward()\n",
    "        nn.utils.clip_grad_norm_(self.policy_net.parameters(), 2)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def render(self):\n",
    "        self.env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c5e87b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'LunarLander-v2'\n",
    "lander = Agent(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bade4d1a",
   "metadata": {},
   "source": [
    "### Exploration profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab774cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define exploration profile\n",
    "max_value = 1\n",
    "num_iterations = 2000\n",
    "exp_decay = 0.01\n",
    "exploration_profile = [max_value * np.exp(-exp_decay * i) for i in range(num_iterations)]\n",
    "exploration_profile = [0.01 if x < 0.01 else x for x in exploration_profile] # set minimum exploring rate\n",
    "\n",
    "\n",
    "### Plot exploration profile\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(exploration_profile)\n",
    "plt.grid()\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Exploration profile (Epsilon)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1a524d",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510ed76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not trained:    \n",
    "    returns = []\n",
    "    mean_ret = []\n",
    "    patience = 100\n",
    "    for num_eps, epsilon in enumerate(tqdm(exploration_profile)):\n",
    "        state = lander.reset()\n",
    "        score = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = lander.choose_action(state, epsilon)\n",
    "            next_state, reward, done = lander.step(action)\n",
    "            score += reward\n",
    "            if done:\n",
    "                next_state = None\n",
    "                returns.append(score)\n",
    "                mean_ret.append(sum(returns[-100:]) / len(returns[-100:]))\n",
    "                if mean_ret[-1] >= 200:\n",
    "                    patience -= 1\n",
    "                else: patience = 100\n",
    "            lander.store(state, action, reward, next_state)\n",
    "            # Update the network\n",
    "            if len(lander.buffer) > min_samples_for_training: # we enable the training only if we have enough samples in the replay memory, otherwise the training will use the same samples too often\n",
    "                lander.learn(loss_function)\n",
    "            state = next_state\n",
    "        if patience == 0:\n",
    "            print(\"The game has been solved after \" + num_eps + \" episodes\")\n",
    "            break\n",
    "        # Update the target network every target_net_update_steps episodes\n",
    "        if num_eps % update_target_every_steps == 0:\n",
    "            print('Updating target network...')\n",
    "            lander.target_net.load_state_dict(lander.policy_net.state_dict())\n",
    "            #torch.save(lander.target_net.state_dict(), 'LunarLander-DQL.torch')\n",
    "\n",
    "        # Print the final score\n",
    "        print(f\"EPISODE: {num_eps + 1} - FINAL SCORE: {score} - Epsilon: {epsilon}\") # Print the final score\n",
    "\n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.plot(mean_ret, 'r')\n",
    "    plt.plot(returns, 'b', alpha = 0.3)\n",
    "    plt.ylabel('Score', fontsize=18)\n",
    "    plt.xlabel('Episode', fontsize=18)\n",
    "    plt.savefig(\"Score_lunar.pdf\", format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd746c16",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4fdac30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-15 15:47:55.463 python[14807:1221039] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fdd667c2c90>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-03-15 15:47:55.464 python[14807:1221039] Warning: Expected min height of view: (<NSButton: 0x7fdd6821e1c0>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-03-15 15:47:55.467 python[14807:1221039] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fdd6821ec80>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-03-15 15:47:55.469 python[14807:1221039] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fdd68220390>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE 1 - FINAL SCORE: 248.9517496563277\n",
      "EPISODE 2 - FINAL SCORE: 282.9224762847525\n",
      "EPISODE 3 - FINAL SCORE: 124.70022897603037\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fy/lwqg7txn3_3516ydtz3kt3x00000gn/T/ipykernel_14807/2349062345.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlander\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0;31m# Visually render the environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m       \u001b[0mlander\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m       \u001b[0;31m# Update the final score (+1 for each step)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m       \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/fy/lwqg7txn3_3516ydtz3kt3x00000gn/T/ipykernel_14807/3635782660.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.8/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"human\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.8/site-packages/gym/envs/box2d/lunar_lander.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    440\u001b[0m             )\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"rgb_array\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.8/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, return_rgb_array)\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mgeom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgeom\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monetime_geoms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0mgeom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.8/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0mattr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.8/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36mdisable\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0mglPopMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_translation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize the Gym environment\n",
    "env_name = 'LunarLander-v2'\n",
    "lander = Agent(env_name)\n",
    "lander.policy_net.load_state_dict(torch.load('LunarLander-DQL.torch', map_location=device))\n",
    "returns = []\n",
    "\n",
    "# Let's try for a total of 10 episodes\n",
    "for num_episode in range(100): \n",
    "    # Reset the environment and get the initial state\n",
    "    state = lander.reset()\n",
    "    # Reset the score. The final score will be the total amount of steps before the pole falls\n",
    "    score = 0\n",
    "    done = False\n",
    "    # Go on until the pole falls off or the score reach 490\n",
    "    while not done:\n",
    "      # Choose the best action (epsilon 0)\n",
    "      action = lander.choose_action(state, epsilon=0)\n",
    "      # Apply the action and get the next state, the reward and a flag \"done\" that is True if the game is ended\n",
    "      next_state, reward, done = lander.step(action)\n",
    "      # Visually render the environment\n",
    "      lander.render()\n",
    "      # Update the final score (+1 for each step)\n",
    "      score += reward \n",
    "      # Set the current state for the next iteration\n",
    "      state = next_state\n",
    "      # Check if the episode ended (the pole fell down)\n",
    "    # Print the final score\n",
    "    returns.append(score)\n",
    "    print(f\"EPISODE {num_episode + 1} - FINAL SCORE: {score}\") \n",
    "print(f\"MEAN SCORE OVER 100 EPISODES IS: {np.mean(returns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1398368b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
